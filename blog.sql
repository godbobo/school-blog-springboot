/*
 Navicat Premium Data Transfer

 Source Server         : 本地数据库
 Source Server Type    : MySQL
 Source Server Version : 50558
 Source Host           : localhost:3306
 Source Schema         : blog

 Target Server Type    : MySQL
 Target Server Version : 50558
 File Encoding         : 65001

 Date: 06/05/2019 17:21:55
*/

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for article
-- ----------------------------
DROP TABLE IF EXISTS `article`;
CREATE TABLE `article`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `content` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `hide` int(11) NOT NULL,
  `summary` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `title` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `top` int(11) NOT NULL,
  `upt` datetime NOT NULL,
  `view` bigint(20) NOT NULL,
  `author` bigint(20) NOT NULL,
  `topic_id` bigint(20) NULL DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `FKdw5d9vdw43e3nvtpqk8l4iitp`(`author`) USING BTREE,
  INDEX `FK6x3cr4vpqhjktvuju4u1f77q1`(`topic_id`) USING BTREE,
  CONSTRAINT `FK6x3cr4vpqhjktvuju4u1f77q1` FOREIGN KEY (`topic_id`) REFERENCES `topic` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKdw5d9vdw43e3nvtpqk8l4iitp` FOREIGN KEY (`author`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB AUTO_INCREMENT = 14 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of article
-- ----------------------------
INSERT INTO `article` VALUES (1, '既然要用Python 3开发爬虫，那么第一步一定是安装Python 3。这里会介绍Windows、Linux和Mac三大平台下的安装过程。\n\n## 1. 相关链接\n\n- 官方网站：[http://python.org](http://python.org/)\n- 下载地址：<https://www.python.org/downloads>\n- 第三方库：<https://pypi.python.org/pypi>\n- 官方文档：<https://docs.python.org/3>\n- 中文教程：<http://www.runoob.com/python3/python3-tutorial.html>\n- Awesome Python：<https://github.com/vinta/awesome-python>\n- Awesome Python中文版：<https://github.com/jobbole/awesome-python-cn>\n\n## 2. Windows下的安装\n\n在Windows下安装Python 3的方式有两种。\n\n- 一种是通过Anaconda安装，它提供了Python的科学计算环境，里面自带了Python以及常用的库。如果选用了这种方式，后面的环境配置方式会更加简便。\n- 另一种是直接下载安装包安装，即标准的安装方式。\n\n下面我们依次介绍这两种安装方式，任选其一即可。\n\n### (1) Anaconda安装\n\nAnaconda的官方下载链接为<https://www.continuum.io/downloads>，选择Python 3版本的安装包下载即可，如图1-1所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708002eb50fe93aef25)\n\n图1-1 Anaconda Windows下载页面\n\n如果下载速度过慢，可以选择使用清华大学镜像，下载列表链接为<https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/>，使用说明链接为<https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/>。\n\n下载完成之后，直接双击安装包安装即可。安装完成之后，Python 3的环境就配置好了。\n\n### (2) 安装包安装\n\n我们推荐直接下载安装包来安装，此时可以直接到官方网站下载Python 3的安装包：<https://www.python.org/downloads/>。\n\n写书时，Python的最新版本1是3.6.2，其下载链接为<https://www.python.org/downloads/release/python-362/>，下载页面如图1-2所示。需要说明的是，实际的Python最新版本以官网为准。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=170831c80abe71970c38)\n\n图1-2 Python下载页面\n\n> 1. 若无特别说明，书中的最新版本均为作者写书时的情况，后面不再一一说明。\n\n64位系统可以下载Windows x86-64 executable installer，32位系统可以下载Windows x86 executable installer。\n\n下载完成之后，直接双击Python安装包，然后通过图形界面安装，接着设置Python的安装路径，完成后将Python 3和Python 3的Scripts目录配置到环境变量即可。\n\n关于环境变量的配置，此处以Windows 10系统为例进行演示。\n\n假如安装后的Python 3路径为C:\\Python36，从资源管理器中打开该路径，如图1-3所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=17082965ee593cae2365)\n\n图1-3 Python安装目录\n\n将该路径复制下来。\n\n随后，右击“计算机”，从中选择“属性”，此时将打开系统属性窗口，如图1-4所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708cdd34354955d1fde)\n\n图1-4 系统属性\n\n点击左侧的“高级系统设置”，即可看到在弹出的对话框下方看到“环境变量”按钮，如图1-5所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708d35844164e63b26a)\n\n图1-5 高级系统设置\n\n点击“环境变量”按钮，找到系统变量下的Path变量，随后点击“编辑”按钮，如图1-6所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708e0b523729330b0e8)\n\n图1-6 环境变量\n\n随后点击“新建”，新建一个条目，将刚才复制的C:\\Python36复制进去。这里需要说明的是，此处的路径就是你的Python 3安装目录，请自行替换。然后，再把C:\\Python36\\Scripts路径复制进去，如图1-7所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708d8f0669ffcf518a7)\n\n图1-7 编辑环境变量\n\n最后，点击“确定”按钮即可完成环境变量的配置。\n\n配置好环境变量后，我们就可以在命令行中直接执行环境变量路径下的可执行文件了，如`python`、`pip`等命令。\n\n### (3) 添加别名\n\n上面这两种安装方式任选其一即可完成安装，但如果之前安装过Python 2的话，可能会导致版本冲突问题，比如在命令行下输入`python`就不知道是调用的Python 2还是Python 3了。为了解决这个问题，建议将安装目录中的python.exe复制一份，命名为python3.exe，这样便可以调用`python3`命令了。实际上，它和`python`命令是完全一致的，这样只是为了可以更好地区分Python版本。当然，如果没有安装过Python 2的话，也建议添加此别名，添加完毕之后的效果如图1-8所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=17089ea4c3d0be341d8e)\n\n图1-8 添加别名\n\n对于pip来说，安装包中自带了pip3.exe可执行文件，我们也可以直接使用`pip3`命令，无需额外配置。\n\n### (4) 测试验证\n\n安装完成后，可以通过命令行测试一下安装是否成功。在“开始”菜单中搜索`cmd`，找到命令提示符，此时就进入命令行模式了。输入`python`，测试一下能否成功调用Python。如果添加了别名的话，可以输入`python3`测试，这里输入的是`python3`，测试结果如图1-9所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=17089590fa9f68124670)\n\n图1-9 测试验证页面\n\n输出结果类似如下：\n\n```sh\n$ python3Python 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)] on win32Type \"help\", \"copyright\", \"credits\" **or** \"license\" **for** more information.>>> print(\'Hello World\')Hello World>>> exit()$ pip3 -Vpip 9.0.1 from c:\\python36\\lib\\site-packages (python 3.6)\n```\n\n如果出现了类似上面的提示，则证明Python 3和pip 3均安装成功；如果提示命令不存在，那么请检查下环境变量的配置情况。\n\n## 3. Linux下的安装\n\nLinux下的安装方式有多种：命令安装、源码安装和Anaconda安装。\n\n使用源码安装需要自行编译，时间较长。推荐使用系统自带的命令或Anaconda安装，简单、高效。这里分别讲解这3种安装方式。\n\n### (1) 命令行安装\n\n不同的Linux发行版本的安装方式又有不同，在此分别予以介绍。\n\n#### CentOS、Red Hat\n\n如果是CentOS或Red Hat版本，则使用`yum`命令安装即可。\n\n下面列出了Python 3.5和Python 3.4两个版本的安装方法，可以自行选择。\n\nPython 3.5版本：\n\n```sh\nsudo yum install -y https:*//centos7.iuscommunity.org/ius-release.rpm*sudo yum updatesudo yum install -y python35u python35u-libs python35u-devel python35u-pip\n```\n\n执行完毕后，便可以成功安装Python 3.5及pip 3了。\n\nPython 3.4版本：\n\n```sh\nsudo yum groupinstall -y development toolssudo yum install -y epel-release python34-devel  libxslt-devel libxml2-devel openssl-develsudo yum install -y python34sudo yum install -y python34-setuptoolssudo easy_install-3.4 pip\n```\n\n执行完毕后，便可以成功安装Python 3.4及pip 3了。\n\n#### Ubuntu、Debian和Deepin\n\n首先安装Python 3，这里使用`apt-get`安装即可。在安装前，还需安装一些基础库，相关命令如下：\n\n```sh\nsudo apt-get install -y python3-dev build-essential libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-dev libcurl4-openssl-devsudo apt-get install -y python3\n```\n\n执行完上述命令后，就可以成功安装Python 3了。\n\n然后还需要安装pip 3，这里仍然使用`apt-get`安装即可，相关命令如下：\n\n```sh\nsudo apt-get install -y python3-pip\n```\n\n执行完毕后，便可以成功安装Python 3及pip 3了。\n\n### (2) 源码安装\n\n如果命令行的安装方式有问题，还可以下载Python 3源码进行安装。\n\n源码下载地址为<https://www.python.org/ftp/python/>，可以自行选用想要的版本进行安装。这里以Python 3.6.2为例进行说明，安装路径设置为/usr/local/python3。\n\n首先，创建安装目录，相关命令如下：\n\n```sh\nsudo mkdir /usr/local/python3\n```\n\n随后下载安装包并解压进入，相关命令如下：\n\n```sh\nwget --no-check-certificate https:*//www.python.org/ftp/python/3.6.2/Python-3.6.2.tgz*tar -xzvf Python-3.6.2.tgzcd Python-3.6.2\n```\n\n接下来，编译安装。所需的时间可能较长，请耐心等待，命令如下：\n\n```sh\nsudo ./configure --prefix=/usr/local/python3sudo makesudo make install\n```\n\n安装完成之后，创建Python 3链接，相关命令如下：\n\n```sh\nsudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3\n```\n\n随后下载pip安装包并安装，命令如下：\n\n```sh\nwget --no-check-certificate https:*//github.com/pypa/pip/archive/9.0.1.tar.gz*tar -xzvf 9.0.1.tar.gzcd pip-9.0.1python3 setup.py install\n```\n\n安装完成后再创建pip 3链接，相关命令如下：\n\n```sh\nsudo ln -s /usr/local/python3/bin/pip /usr/bin/pip3\n```\n\n这样就成功安装好了Python 3及pip 3。\n\n### (3) Anaconda安装\n\nAnaconda同样支持Linux，其官方下载链接为[https://www.continuum.io/downloads](http://epub.ituring.com.cn/article/edit/[https://www.continuum.io/downloads)，选择Python 3版本的安装包下载即可，如图1-10所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708a13922b218a1bbb2)\n\n图1-10 Anaconda Linux下载页面\n\n如果下载速度过慢，同样可以使用清华镜像，具体可参考Windows部分的介绍，在此不再赘述。\n\n### (4) 测试验证\n\n在命令行界面下测试Python 3和pip 3是否安装成功：\n\n```sh\n$ python3Python 3.5.2 (**default**, Nov 17 2016, 17:05:23) Type \"help\", \"copyright\", \"credits\" **or** \"license\" **for** more information.>>> exit()$ pip3 -Vpip 8.1.1 from /usr/lib/python3/dist-packages (python 3.5)\n```\n\n若出现类似上面的提示，则证明Python 3和pip 3安装成功。\n\n## 4. Mac下的安装\n\n在Mac下同样有多种安装方式，如Homebrew、安装包安装、Anaconda安装等，这里推荐使用Homebrew安装。\n\n### (1) Homebrew安装\n\nHomebrew是Mac平台下强大的包管理工具，其官方网站是<https://brew.sh/>。\n\n执行如下命令，即可安装Homebrew：\n\n```sh\nruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n```\n\n安装完成后，便可以使用`brew`命令安装Python 3和pip 3了：\n\n```sh\nbrew install python3\n```\n\n命令执行完成后，我们发现Python 3和pip 3均已成功安装。\n\n### (2) 安装包安装\n\n可以到官方网站下载Python 3安装包。链接为<https://www.python.org/downloads/>，页面如图1-2所示。\n\n在Mac平台下，可以选择下载Mac OS X 64-bit/32-bit installer，下载完成后，打开安装包按照提示安装即可。\n\n### (3) Anaconda安装\n\nAnaconda同样支持Mac，其官方下载链接为：[https://www.continuum.io/downloads](http://epub.ituring.com.cn/article/edit/[https://www.continuum.io/downloads)，选择Python 3版本的安装包下载即可，如图1-11所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=17089b034fd975411c37)\n\n图1-11 Anaconda Mac下载页面\n\n如果下载速度过慢，同样可以使用清华镜像，具体可参考Windows部分的介绍，在此不再赘述。\n\n### (4) 测试验证\n\n打开终端，在命令行界面中测试Python 3和pip 3是否成功安装，如图1-12所示。\n\n![图像说明文字](http://epub.ituring.com.cn/api/storage/getbykey/screenshow?key=1708788341be5f65d01e)\n\n图1-12 测试验证页面\n\n若出现上面的提示，则证明Python 3和pip 3安装成功。\n\n本节中，我们介绍了3大平台Windows、Linux和Mac下Python 3的安装方式。安装完成后，我们便可以开启Python爬虫的征程了。', 0, '既然要用Python 3开发爬虫，那么第一步一定是安装Python 3。这里会介绍Windows、Linux和Mac三大平台下的安装过程。', 'Python的安装', 0, '2019-05-06 11:00:31', 0, 1, 1);
INSERT INTO `article` VALUES (2, '由于Requests属于第三方库，也就是Python默认不会自带这个库，所以需要我们手动安装。下面我们首先看一下它的安装过程。\n\n## 1. 相关链接\n\n- GitHub：<https://github.com/requests/requests>\n- PyPI：<https://pypi.python.org/pypi/requests>\n- 官方文档：[http://www.python-requests.org](http://www.python-requests.org/)\n- 中文文档：<http://docs.python-requests.org/zh_CN/latest>\n\n## 2. pip安装\n\n无论是Windows、Linux还是Mac，都可以通过pip这个包管理工具来安装。\n\n在命令行界面中运行如下命令，即可完成Requests库的安装：\n\n```bash\npip3 install requests\n```\n\n这是最简单的安装方式，推荐使用这种方法安装。\n\n## 3. wheel安装\n\nwheel是Python的一种安装包，其后缀为.whl，在网速较差的情况下可以选择下载wheel文件再安装，然后直接用`pip3`命令加文件名安装即可。\n\n不过在这之前需要先安装wheel库，安装命令如下：\n\n```sh\npip3 install wheel\n```\n\n然后到PyPI上下载对应的wheel文件，如最新版本为2.17.3，则打开<https://pypi.python.org/pypi/requests/2.17.3#downloads>，下载requests-2.17.3-py2.py3-none-any.whl到本地。\n\n随后在命令行界面进入wheel文件目录，利用`pip`安装即可：\n\n```sh\npip3 install requests-2.17.3-py2.py3-none-any.whl\n```\n\n这样我们也可以完成Requests的安装。\n\n## 4. 源码安装\n\n如果你不想用pip来安装，或者想获取某一特定版本，可以选择下载源码安装。\n\n此种方式需要先找到此库的源码地址，然后下载下来再用命令安装。\n\nRequests项目的地址是：<https://github.com/kennethreitz/requests>。\n\n可以通过Git来下载源代码：\n\n```sh\ngit clone git://github.com/kennethreitz/requests.git\n```\n\n或通过`curl`下载：\n\n```sh\ncurl -OL https://github.com/kennethreitz/requests/tarball/master\n```\n\n下载下来之后，进入目录，执行如下命令即可安装：\n\n```sh\ncd requestspython3 setup.py install\n```\n\n命令执行结束后即可完成Requests的安装。由于这种安装方式比较烦琐，后面不再赘述。\n\n## 5. 验证安装\n\n为了验证库是否已经安装成功，可以在命令行模式测试一下：\n\n```sh\n$ python3>>> import requests\n```\n\n首先输入`python3`，进入命令行模式，然后输入上述内容，如果什么错误提示也没有，就证明已经成功安装了Requests。', 0, '由于Requests属于第三方库，也就是Python默认不会自带这个库，所以需要我们手动安装。下面我们首先看一下它的安装过程。', 'Requests的安装', 0, '2019-05-06 11:01:15', 1, 1, 1);
INSERT INTO `article` VALUES (3, 'lxml是Python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高。本节中，我们了解一下lxml的安装方式，这主要从Windows、Linux和Mac三大平台来介绍。\n\n## 1. 相关链接\n\n- 官方网站：[http://lxml.de](http://lxml.de/)\n- GitHub：<https://github.com/lxml/lxml>\n- PyPI：<https://pypi.python.org/pypi/lxml>\n\n## 2. Windows下的安装\n\n在Windows下，可以先尝试利用pip安装，此时直接执行如下命令即可：\n\n```sh\npip3 install lxml\n```\n\n如果没有任何报错，则证明安装成功。\n\n如果出现报错，比如提示缺少libxml2库等信息，可以采用wheel方式安装。\n\n推荐直接到这里（链接为：<http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml>）下载对应的wheel文件，找到本地安装Python版本和系统对应的lxml版本，例如Windows 64位、Python 3.6，就选择lxml‑3.8.0‑cp36‑cp36m‑win_amd64.whl，将其下载到本地。\n\n然后利用pip安装即可，命令如下：\n\n```sh\npip3 install lxml‑3.8.0‑cp36‑cp36m‑win_amd64.whl\n```\n\n这样我们就可以成功安装lxml了。\n\n## 3. Linux下的安装\n\n在Linux平台下安装问题不大，同样可以先尝试pip安装，命令如下：\n\n```sh\npip3 install lxml\n```\n\n如果报错，可以尝试下面的解决方案。\n\n### CentOS、Red Hat\n\n对于此类系统，报错主要是因为缺少必要的库。\n\n执行如下命令安装所需的库即可：\n\n```sh\nsudo yum groupinstall -y development toolssudo yum install -y epel-release libxslt-devel libxml2-devel openssl-devel\n```\n\n主要是libxslt-devel和libxml2-devel这两个库，lxml依赖它们。安装好之后，重新尝试pip安装即可。\n\n### Ubuntu、Debian和Deepin\n\n在这些系统下，报错的原因同样可能是缺少了必要的类库，执行如下命令安装：\n\n```sh\nsudo apt-get install -y python3-dev build-essential libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-dev\n```\n\n安装好之后，重新尝试pip安装即可。\n\n## 4. Mac下的安装\n\n在Mac平台下，仍然可以首先尝试pip安装，命令如下：\n\n```sh\npip3 install lxml\n```\n\n如果产生错误，可以执行如下命令将必要的类库安装：\n\n```sh\nxcode-select --install\n```\n\n之后再重新尝试pip安装，就没有问题了。\n\nlxml是一个非常重要的库，后面的Beautiful Soup、Scrapy框架都需要用到此库，所以请一定安装成功。\n\n## 5. 验证安装\n\n安装完成之后，可以在Python命令行下测试：\n\n```sh\n$ python3>>> import lxml\n```\n\n如果没有错误报出，则证明库已经安装好了。', 0, 'lxml是Python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高。本节中，我们了解一下lxml的安装方式，这主要从Windows、Linux和Mac三大平台来介绍。', 'Lxml的安装', 0, '2019-05-06 11:02:04', 0, 1, 1);
INSERT INTO `article` VALUES (4, 'Beautiful Soup是Python的一个HTML或XML的解析库，我们可以用它来方便地从网页中提取数据。它拥有强大的API和多样的解析方式，本节就来了解下它的安装方式。\n\n## 1. 相关链接\n\n- 官方文档：<https://www.crummy.com/software/BeautifulSoup/bs4/doc>\n- 中文文档：<https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh>\n- PyPI：<https://pypi.python.org/pypi/beautifulsoup4>\n\n## 2. 准备工作\n\nBeautiful Soup的HTML和XML解析器是依赖于lxml库的，所以在此之前请确保已经成功安装好了lxml库，具体的安装方式参见上节。\n\n## 3. pip安装\n\n目前，Beautiful Soup的最新版本是4.x版本，之前的版本已经停止开发了。这里推荐使用pip来安装，安装命令如下：\n\n```sh\npip3 install beautifulsoup4\n```\n\n命令执行完毕之后即可完成安装。\n\n## 4. wheel安装\n\n当然，我们也可以从PyPI下载wheel文件安装，链接如下：<https://pypi.python.org/pypi/beautifulsoup4>\n\n然后使用pip安装wheel文件即可。\n\n## 5. 验证安装\n\n安装完成之后，可以运行下面的代码验证一下：\n\n```sh\nfrom bs4 import BeautifulSoupsoup = BeautifulSoup(\'<p>Hello</p>\', \'lxml\')print(soup.p.string)\n```\n\n运行结果如下：\n\n```sh\nHello\n```\n\n如果运行结果一致，则证明安装成功。\n\n注意，这里我们虽然安装的是beautifulsoup4这个包，但是在引入的时候却是bs4。这是因为这个包源代码本身的库文件夹名称就是bs4，所以安装完成之后，这个库文件夹就被移入到本机Python3的lib库里，所以识别到的库文件名就叫作bs4。\n\n因此，包本身的名称和我们使用时导入的包的名称并不一定是一致的。', 0, 'Beautiful Soup是Python的一个HTML或XML的解析库，我们可以用它来方便地从网页中提取数据。它拥有强大的API和多样的解析方式，本节就来了解下它的安装方式。', 'Beautiful Soup的安装', 0, '2019-05-06 11:02:48', 0, 1, 1);
INSERT INTO `article` VALUES (5, '在本节中，我们会详细了解HTTP的基本原理，了解在浏览器中敲入URL到获取网页内容之间发生了什么。了解了这些内容，有助于我们进一步了解爬虫的基本原理。\n\n## 1. URI和URL\n\n这里我们先了解一下URI和URL，URI的全称为Uniform Resource Identifier，即统一资源标志符，URL的全称为Universal Resource Locator，即统一资源定位符。\n\n举例来说，<https://github.com/favicon.ico>是GitHub的网站图标链接，它是一个URL，也是一个URI。即有这样的一个图标资源，我们用URL/URI来唯一指定了它的访问方式，这其中包括了访问协议https、访问路径（/即根目录）和资源名称favicon.ico。通过这样一个链接，我们便可以从互联网上找到这个资源，这就是URL/URI。\n\nURL是URI的子集，也就是说每个URL都是URI，但不是每个URI都是URL。那么，怎样的URI不是URL呢？URI还包括一个子类叫作URN，它的全称为Universal Resource Name，即统一资源名称。URN只命名资源而不指定如何定位资源，比如urn:isbn:0451450523指定了一本书的ISBN，可以唯一标识这本书，但是没有指定到哪里定位这本书，这就是URN。URL、URN和URI的关系可以用图2-1表示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-1.jpg)图2-1 URL、URN和URI关系图\n\n但是在目前的互联网中，URN用得非常少，所以几乎所有的URI都是URL，一般的网页链接我们既可以称为URL，也可以称为URI，我个人习惯称为URL。\n\n## 2.  超文本\n\n接下来，我们再了解一个概念——超文本，其英文名称叫作hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列HTML代码，里面包含了一系列标签，比如`img`显示图片，`p`指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。\n\n例如，我们在Chrome浏览器里面打开任意一个页面，如淘宝首页，右击任一地方并选择“检查”项（或者直接按快捷键F12），即可打开浏览器的开发者工具，这时在Elements选项卡即可看到当前网页的源代码，这些源代码都是超文本，如图2-2所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-2.png)图2-2 源代码\n\n## 3. HTTP和HTTPS\n\n在淘宝的首页<https://www.taobao.com/>中，URL的开头会有http或https，这就是访问资源需要的协议类型。有时，我们还会看到ftp、sftp、smb开头的URL，它们都是协议类型。在爬虫中，我们抓取的页面通常就是http或https协议的，这里首先了解一下这两个协议的含义。\n\nHTTP的全称是Hyper Text Transfer Protocol，中文名叫作超文本传输协议。HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet Engineering Task Force）共同合作制定的规范，目前广泛使用的是HTTP 1.1版本。\n\nHTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。\n\nHTTPS的安全基础是SSL，因此通过它传输的内容都是经过SSL加密的，它的主要作用可以分为两种。\n\n- 建立一个信息安全通道来保证数据传输的安全。\n- 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。\n\n现在越来越多的网站和App都已经向HTTPS方向发展，例如：\n\n- 苹果公司强制所有iOS App在2017年1月1日前全部改为使用HTTPS加密，否则App就无法在应用商店上架；\n- 谷歌从2017年1月推出的Chrome 56开始，对未进行HTTPS加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户“此网页不安全”；\n- 腾讯微信小程序的官方需求文档要求后台使用HTTPS请求进行网络通信，不满足条件的域名和协议无法请求。\n\n而某些网站虽然使用了HTTPS协议，但还是会被浏览器提示不安全，例如我们在Chrome浏览器里面打开12306，链接为：<https://www.12306.cn/>，这时浏览器就会提示“您的连接不是私密连接”这样的话，如图2-3所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-3.png)图2-3 12306页面\n\n这是因为12306的CA证书是中国铁道部自行签发的，而这个证书是不被CA机构信任的，所以这里证书验证就不会通过而提示这样的话，但是实际上它的数据传输依然是经过SSL加密的。如果要爬取这样的站点，就需要设置忽略证书的选项，否则会提示SSL链接错误。\n\n## 4. HTTP请求过程\n\n我们在浏览器中输入一个URL，回车之后便会在浏览器中观察到页面内容。实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，模型如图2-4所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-4.jpg)图2-4 模型图\n\n此处客户端即代表我们自己的PC或手机浏览器，服务器即要访问的网站所在的服务器。\n\n为了更直观地地说明这个过程，这里用Chrome浏览器的开发者模式下的Network监听组件来做下演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。\n\n打开Chrome浏览器，右击并选择“检查”项，即可打开浏览器的开发者工具。这里访问百度<http://www.baidu.com/>，输入该URL后回车，观察这个过程中发生了怎样的网络请求。可以看到，在Network页面下方出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图2-5所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-5.png)图2-5 Network面板\n\n我们先观察第一个网络请求，即www.baidu.com。\n\n其中各列的含义如下。\n\n- **第一列Name**：请求的名称，一般会将URL的最后一部分内容当作名称。\n- **第二列Status**：响应的状态码，这里显示为200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。\n- **第三列Type**：请求的文档类型。这里为document，代表我们这次请求的是一个HTML文档，内容就是一些HTML代码。\n- **第四列Initiator**：请求源。用来标记请求是由哪个对象或进程发起的。\n- **第五列Size**：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示from cache。\n- **第六列Time**：发起请求到获取响应所用的总时间。\n- **第七列Waterfall**：网络请求的可视化瀑布流。\n\n点击这个条目，即可看到更详细的信息，如图2-6所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-6.jpg)图2-6 详细信息\n\n首先是General部分，Request URL为请求的URL，Request Method为请求的方法，Status Code为响应状态码，Remote Address为远程服务器的地址和端口，Referrer Policy为Referrer判别策略。\n\n再继续往下看，可以看到，有Response Headers和Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的Response Headers就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。\n\n下面我们分别来介绍一下请求和响应都包含哪些内容。\n\n## 5. 请求\n\n请求，由客户端向服务端发出，可以分为4部分内容：请求方法（Request Method）、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。\n\n### (1) 请求方法\n\n常见的请求方法有两种：GET和POST。\n\n在浏览器中直接输入URL并回车，这便发起了一个GET请求，请求的参数会直接包含到URL里。例如，在百度中搜索Python，这就是一个GET请求，链接为<https://www.baidu.com/s?wd=Python>，其中URL中包含了请求的参数信息，这里参数`wd`表示要搜寻的关键字。POST请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，其数据通常以表单的形式传输，而不会体现在URL中。\n\nGET和POST请求方法有如下区别。\n\n- GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。\n- GET请求提交的数据最多只有1024字节，而POST方式没有限制。\n\n一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。\n\n我们平常遇到的绝大部分请求都是GET或POST请求，另外还有一些请求方法，如GET、HEAD、POST、PUT、DELETE、OPTIONS、CONNECT、TRACE等，我们简单将其总结为表2-1。\n\n表2-1 其他请求方法\n\n|  方法   |                             描述                             |\n| :-----: | :----------------------------------------------------------: |\n|   GET   |                   请求页面，并返回页面内容                   |\n|  HEAD   | 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 |\n|  POST   |        大多用于提交表单或上传文件，数据包含在请求体中        |\n|   PUT   |        从客户端向服务器传送的数据取代指定文档中的内容        |\n| DELETE  |                   请求服务器删除指定的页面                   |\n| CONNECT |       把服务器当作跳板，让服务器代替客户端访问其他网页       |\n| OPTIONS |                  允许客户端查看服务器的性能                  |\n|  TRACE  |           回显服务器收到的请求，主要用于测试或诊断           |\n\n本表参考：<http://www.runoob.com/http/http-methods.html>。\n\n### (2) 请求的网址\n\n请求的网址，即统一资源定位符URL，它可以唯一确定我们想请求的资源。\n\n### (3) 请求头\n\n请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie、Referer、User-Agent等。下面简要说明一些常用的头信息。\n\n- **Accept**：请求报头域，用于指定客户端可接受哪些类型的信息。\n- **Accept-Language**：指定客户端可接受的语言类型。\n- **Accept-Encoding**：指定客户端可接受的内容编码。\n- **Host**：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从HTTP 1.1版本开始，请求必须包含此内容。\n- **Cookie**：也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。\n- **Referer**：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如作来源统计、防盗链处理等。\n- **User-Agent**：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。\n- **Content-Type**：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型，更多对应关系可以查看此对照表：<http://tool.oschina.net/commons>。\n\n因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。\n\n### (4) 请求体\n\n请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。\n\n例如，这里我登录GitHub时捕获到的请求和响应如图2-7所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-7.jpg)图2-7 详细信息\n\n登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意Request Headers中指定Content-Type为application/x-www-form-urlencoded。只有设置Content-Type为application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将Content-Type设置为application/json来提交JSON数据，或者设置为multipart/form-data来上传文件。\n\n表2-2列出了Content-Type和POST提交数据方式的关系。\n\n表2-2 Content-Type和POST提交数据方式的关系\n\n|           Content-Type            | 提交数据的方式 |\n| :-------------------------------: | :------------: |\n| application/x-www-form-urlencoded |    表单数据    |\n|        multipart/form-data        |  表单文件上传  |\n|         application/json          | 序列化JSON数据 |\n|             text/xml              |    XML数据     |\n\n在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type，不然可能会导致POST提交后无法正常响应。\n\n## 6. 响应\n\n响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。\n\n### (1) 响应状态码\n\n响应状态码表示服务器的响应状态，如200代表服务器正常响应，404代表页面未找到，500代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为200，则证明成功返回数据，再进行进一步的处理，否则直接忽略。表2-3列出了常见的错误代码及错误原因。\n\n表2-3 常见的错误代码及错误原因\n\n| 状态码 |      说明      |                             详情                             |\n| :----: | :------------: | :----------------------------------------------------------: |\n|  100   |      继续      | 请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分 |\n|  101   |    切换协议    |      请求者已要求服务器切换协议，服务器已确认并准备切换      |\n|  200   |      成功      |                    服务器已成功处理了请求                    |\n|  201   |     已创建     |               请求成功并且服务器创建了新的资源               |\n|  202   |     已接受     |                 服务器已接受请求，但尚未处理                 |\n|  203   |   非授权信息   |     服务器已成功处理了请求，但返回的信息可能来自另一个源     |\n|  204   |     无内容     |           服务器成功处理了请求，但没有返回任何内容           |\n|  205   |    重置内容    |               服务器成功处理了请求，内容被重置               |\n|  206   |    部分内容    |                   服务器成功处理了部分请求                   |\n|  300   |    多种选择    |                针对请求，服务器可执行多种操作                |\n|  301   |    永久移动    |          请求的网页已永久移动到新位置，即永久重定向          |\n|  302   |    临时移动    |          请求的网页暂时跳转到其他页面，即暂时重定向          |\n|  303   |  查看其他位置  |     如果原来的请求是POST，重定向目标文档应该通过GET提取      |\n|  304   |     未修改     |         此次请求返回的网页未修改，继续使用上次的资源         |\n|  305   |    使用代理    |                 请求者应该使用代理访问该网页                 |\n|  307   |   临时重定向   |                 请求的资源临时从其他位置响应                 |\n|  400   |    错误请求    |                     服务器无法解析该请求                     |\n|  401   |     未授权     |               请求没有进行身份验证或验证未通过               |\n|  403   |    禁止访问    |                       服务器拒绝此请求                       |\n|  404   |     未找到     |                    服务器找不到请求的网页                    |\n|  405   |    方法禁用    |                 服务器禁用了请求中指定的方法                 |\n|  406   |     不接受     |               无法使用请求的内容响应请求的网页               |\n|  407   |  需要代理授权  |                    请求者需要使用代理授权                    |\n|  408   |    请求超时    |                        服务器请求超时                        |\n|  409   |      冲突      |                  服务器在完成请求时发生冲突                  |\n|  410   |     已删除     |                     请求的资源已永久删除                     |\n|  411   |  需要有效长度  |          服务器不接受不含有效内容长度标头字段的请求          |\n|  412   | 未满足前提条件 |       服务器未满足请求者在请求中设置的其中一个前提条件       |\n|  413   |  请求实体过大  |              请求实体过大，超出服务器的处理能力              |\n|  414   |  请求URI过长   |                 请求网址过长，服务器无法处理                 |\n|  415   |   不支持类型   |                   请求格式不被请求页面支持                   |\n|  416   |  请求范围不符  |                    页面无法提供请求的范围                    |\n|  417   |  未满足期望值  |              服务器未满足期望请求标头字段的要求              |\n|  500   | 服务器内部错误 |                 服务器遇到错误，无法完成请求                 |\n|  501   |     未实现     |                  服务器不具备完成请求的功能                  |\n|  502   |    错误网关    |        服务器作为网关或代理，从上游服务器收到无效响应        |\n|  503   |   服务不可用   |                      服务器目前无法使用                      |\n|  504   |    网关超时    |    服务器作为网关或代理，但是没有及时从上游服务器收到请求    |\n|  505   | HTTP版本不支持 |             服务器不支持请求中所用的HTTP协议版本             |\n\n### (2) 响应头\n\n响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。下面简要说明一些常用的头信息。\n\n- **Date**：标识响应产生的时间。\n- **Last-Modified**：指定资源的最后修改时间。\n- **Content-Encoding**：指定响应内容的编码。\n- **Server**：包含服务器的信息，比如名称、版本号等。\n- **Content-Type**：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。\n- **Set-Cookie**：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。\n- **Expires**：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。\n\n### (3) 响应体\n\n最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图2-8所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-8.jpg)图2-8 响应体内容\n\n在浏览器开发者工具中点击Preview，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。\n\n在做爬虫时，我们主要通过响应体得到网页的源代码、JSON数据等，然后从中做相应内容的提取。\n\n本节中，我们了解了HTTP的基本原理，大概了解了访问网页时背后的请求和响应过程。本节涉及的知识点需要好好掌握，后面分析网页请求时会经常用到。', 0, '在本节中，我们会详细了解HTTP的基本原理，了解在浏览器中敲入URL到获取网页内容之间发生了什么。了解了这些内容，有助于我们进一步了解爬虫的基本原理。', 'HTTP基本原理', 0, '2019-05-06 11:03:24', 0, 1, 1);
INSERT INTO `article` VALUES (6, '用浏览器访问网站时，页面各不相同，你有没有想过它为何会呈现这个样子呢？本节中，我们就来了解一下网页的基本组成、结构和节点等内容。\n\n## 1. 网页的组成\n\n网页可以分为三大部分——HTML、CSS和JavaScript。如果把网页比作一个人的话，HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤，三者结合起来才能形成一个完善的网页。下面我们分别来介绍一下这三部分的功能。\n\n### (1) HTML\n\nHTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是HTML。不同类型的文字通过不同类型的标签来表示，如图片用`img`标签表示，视频用`video`标签表示，段落用`p`标签表示，它们之间的布局又常通过布局标签`div`嵌套组合而成，各种标签通过不同的排列和嵌套才形成了网页的框架。\n\n在Chrome浏览器中打开百度，右击并选择“检查”项（或按F12键），打开开发者模式，这时在Elements选项卡中即可看到网页的源代码，如图2-9所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-10.png)图2-9 源代码\n\n这就是HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。\n\n### (2) CSS\n\nHTML定义了网页的结构，但是只有HTML页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里借助了CSS。\n\nCSS，全称叫作Cascading Style Sheets，即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等格式。\n\nCSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。\n\n图2-9的右侧即为CSS，例如：\n\n```css\n#head_wrapper.s-ps-islite .s-p-top {\n    position: absolute;\n    bottom: 40px;\n    width: 100%;\n    height: 181px;\n}\n```\n\n就是一个CSS样式。大括号前面是一个CSS选择器，此选择器的意思是首先选中`id`为`head_wrapper`且`class`为`s-ps-islite`的节点，然后再选中其内部的`class`为`s-p-top`的节点。大括号内部写的就是一条条样式规则，例如`position`指定了这个元素的布局方式为绝对布局，`bottom`指定元素的下边距为40像素，`width`指定了宽度为100%占满父元素，`height`则指定了元素的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上CSS选择器，这就代表这个样式对CSS选择器选中的元素生效，元素就会根据此样式来展示了。\n\n在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中（其后缀为css）。在HTML中，只需要用`link`标签即可引入写好的CSS文件，这样整个页面就会变得美观、优雅。\n\n### (3) JavaScript\n\nJavaScript，简称JS，是一种脚本语言。HTML和CSS配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。\n\nJavaScript通常也是以单独的文件形式加载的，后缀为js，在HTML中通过`script`标签即可引入，例如：\n\n```html\n<script src=\"jquery-2.1.0.js\"></script>\n```\n\n综上所述，HTML定义了网页的内容和结构，CSS描述了网页的布局，JavaScript定义了网页的行为。\n\n## 2. 网页的结构\n\n我们首先用例子来感受一下HTML的基本结构。新建一个文本文件，名称可以自取，后缀为html，内容如下：\n\n```html\n<!DOCTYPE html>\n<html>\n    \n    <head>\n        <meta charset=\"UTF-8\">\n        <title>This is a Demo</title>\n    </head>\n    \n    <body>\n        <div id=\"container\">\n            <div class=\"wrapper\">\n                 <h2 class=\"title\">Hello World</h2> \n                <p class=\"text\">Hello, this is a paragraph.</p>\n            </div>\n        </div>\n    </body>\n\n</html>\n```\n\n这就是一个最简单的HTML实例。开头用`DOCTYPE`定义了文档类型，其次最外层是`html`标签，最后还有对应的结束标签来表示闭合，其内部是`head`标签和`body`标签，分别代表网页头和网页体，它们也需要结束标签。`head`标签内定义了一些页面的配置和引用，如：\n\n```html\n<meta charset=\"UTF-8\">\n```\n\n它指定了网页的编码为UTF-8。\n\n`title`标签则定义了网页的标题，会显示在网页的选项卡中，不会显示在正文中。`body`标签内则是在网页正文中显示的内容。`div`标签定义了网页中的区块，它的`id`是`container`，这是一个非常常用的属性，且`id`的内容在网页中是唯一的，我们可以通过它来获取这个区块。然后在此区块内又有一个`div`标签，它的`class`为`wrapper`，这也是一个非常常用的属性，经常与CSS配合使用来设定样式。然后此区块内部又有一个`h2`标签，这代表一个二级标题。另外，还有一个`p`标签，这代表一个段落。在这两者中直接写入相应的内容即可在网页中呈现出来，它们也有各自的`class`属性。\n\n将代码保存后，在浏览器中打开该文件，可以看到如图2-10所示的内容。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-11.png)图2-10 运行结果\n\n可以看到，在选项卡上显示了This is a Demo字样，这是我们在`head`中的`title`里定义的文字。而网页正文是`body`标签内部定义的各个元素生成的，可以看到这里显示了二级标题和段落。\n\n这个实例便是网页的一般结构。一个网页的标准形式是`html`标签内嵌套`head`和`body`标签，`head`内定义网页的配置和引用，`body`内定义网页的正文。\n\n## 3. 节点树及节点间的关系\n\n在HTML中，所有标签定义的内容都是节点，它们构成了一个HTML DOM树。\n\n我们先看下什么是DOM，DOM是W3C（万维网联盟）的标准，其英文全称Document Object Model，即文档对象模型。它定义了访问HTML和XML文档的标准：\n\n> W3C文档对象模型（DOM）是中立于平台和语言的接口，它允许程序和脚本动态地访问和更新文档的内容、结构和样式。\n\nW3C DOM标准被分为3个不同的部分。\n\n- **核心DOM**： 针对任何结构化文档的标准模型。\n- **XML DOM**：针对XML文档的标准模型。\n- **HTML DOM**：针对HTML文档的标准模型。\n\n根据W3C的HTML DOM标准，HTML文档中的所有内容都是节点。\n\n- 整个文档是一个文档节点；\n- 每个HTML元素是元素节点；\n- HTML元素内的文本是文本节点；\n- 每个HTML属性是属性节点；\n- 注释是注释节点。\n\nHTML DOM将HTML文档视作树结构，这种结构被称为节点树，如图2-11所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-12.jpg)图2-11 节点树\n\n通过HTML DOM，树中的所有节点均可通过JavaScript访问，所有HTML节点元素均可被修改，也可以被创建或删除。\n\n节点树中的节点彼此拥有层级关系。我们常用父（parent）、子（child）和兄弟（sibling）等术语描述这些关系。父节点拥有子节点，同级的子节点被称为兄弟节点。\n\n在节点树中，顶端节点称为根（root）。除了根节点之外，每个节点都有父节点，同时可拥有任意数量的子节点或兄弟节点。图2-12展示了节点树以及节点之间的关系。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/2-13.jpg)图2-12 节点树及节点间的关系\n\n本段参考W3SCHOOL，链接：<http://www.w3school.com.cn/htmldom/dom_nodes.asp>。\n\n## 4. 选择器\n\n我们知道网页由一个个节点组成，CSS选择器会根据不同的节点设置不同的样式规则，那么怎样来定位节点呢？\n\n在CSS中，我们使用CSS选择器来定位节点。例如，上例中`div`节点的`id`为`container`，那么就可以表示为`#container`，其中`#`开头代表选择`id`，其后紧跟`id`的名称。另外，如果我们想选择`class`为`wrapper`的节点，便可以使用`.wrapper`，这里以点（.）开头代表选择`class`，其后紧跟`class`的名称。另外，还有一种选择方式，那就是根据标签名筛选，例如想选择二级标题，直接用`h2`即可。这是最常用的3种表示，分别是根据`id`、`class`、标签名筛选，请牢记它们的写法。\n\n另外，CSS选择器还支持嵌套选择，各个选择器之间加上空格分隔开便可以代表嵌套关系，如`#container .wrapper p`则代表先选择`id`为`container`的节点，然后选中其内部的`class`为`wrapper`的节点，然后再进一步选中其内部的`p`节点。另外，如果不加空格，则代表并列关系，如`div#container .wrapper p.text`代表先选择`id`为`container`的`div`节点，然后选中其内部的`class`为`wrapper`的节点，再进一步选中其内部的`class`为`text`的`p`节点。这就是CSS选择器，其筛选功能还是非常强大的。\n\n另外，CSS选择器还有一些其他语法规则，具体如表2-4所示。\n\n表2-4 CSS选择器的其他语法规则\n\n|         选择器         |         例子         |                   例子描述                    |\n| :--------------------: | :------------------: | :-------------------------------------------: |\n|        `.class`        |       `.intro`       |         选择`class=\"intro\"`的所有节点         |\n|         `#id`          |     `#firstname`     |        选择`id=\"firstname\"`的所有节点         |\n|          `*`           |         `*`          |                 选择所有节点                  |\n|       `element`        |         `p`          |                选择所有`p`节点                |\n|   `element,element`    |       `div,p`        |        选择所有`div`节点和所有`p`节点         |\n|   `element element`    |       `div p`        |        选择`div`节点内部的所有`p`节点         |\n|   `element>element`    |       `div>p`        |      选择父节点为`div`节点的所有`p`节点       |\n|   `element+element`    |       `div+p`        |     选择紧接在`div`节点之后的所有`p`节点      |\n|     `[attribute]`      |      `[target]`      |        选择带有`target`属性的所有节点         |\n|  `[attribute=value]`   |   `[target=blank]`   |        选择`target=\"blank\"`的所有节点         |\n|  `[attribute~=value]`  |  `[title~=flower]`   |   选择`title`属性包含单词`flower`的所有节点   |\n|        `:link`         |       `a:link`       |            选择所有未被访问的链接             |\n|       `:visited`       |     `a:visited`      |            选择所有已被访问的链接             |\n|       `:active`        |      `a:active`      |                 选择活动链接                  |\n|        `:hover`        |      `a:hover`       |          选择鼠标指针位于其上的链接           |\n|        `:focus`        |    `input:focus`     |           选择获得焦点的`input`节点           |\n|    `:first-letter`     |   `p:first-letter`   |            选择每个`p`节点的首字母            |\n|     `:first-line`      |    `p:first-line`    |             选择每个`p`节点的首行             |\n|     `:first-child`     |   `p:first-child`    |   选择属于父节点的第一个子节点的所有`p`节点   |\n|       `:before`        |      `p:before`      |        在每个`p`节点的内容之前插入内容        |\n|        `:after`        |      `p:after`       |        在每个`p`节点的内容之后插入内容        |\n|   `:lang(language)`    |       `p:lang`       | 选择带有以`it`开头的`lang`属性值的所有`p`节点 |\n|  `element1~element2`   |        `p~ul`        |        选择前面有`p`节点的所有`ul`节点        |\n|  `[attribute^=value]`  |  `a[src^=\"https\"]`   |  选择其`src`属性值以`https`开头的所有`a`节点  |\n|  `[attribute$=value]`  |   `a[src$=\".pdf\"]`   |   选择其`src`属性以`.pdf`结尾的所有`a`节点    |\n|  `[attribute*=value]`  |   `a[src*=\"abc\"]`    |  选择其`src`属性中包含`abc`子串的所有`a`节点  |\n|    `:first-of-type`    |  `p:first-of-type`   |  选择属于其父节点的首个`p`节点的所有`p`节点   |\n|    `:last-of-type`     |   `p:last-of-type`   |  选择属于其父节点的最后`p`节点的所有`p`节点   |\n|    `:only-of-type`     |   `p:only-of-type`   |  选择属于其父节点唯一的`p`节点的所有`p`节点   |\n|     `:only-child`      |    `p:only-child`    |   选择属于其父节点的唯一子节点的所有`p`节点   |\n|    `:nth-child(n)`     |    `p:nth-child`     |  选择属于其父节点的第二个子节点的所有`p`节点  |\n|  `:nth-last-child(n)`  |  `p:nth-last-child`  |        同上，从最后一个子节点开始计数         |\n|   `:nth-of-type(n)`    |   `p:nth-of-type`    |  选择属于其父节点第二个`p`节点的所有`p`节点   |\n| `:nth-last-of-type(n)` | `p:nth-last-of-type` |      同上，但是从最后一个子节点开始计数       |\n|     `:last-child`      |    `p:last-child`    |  选择属于其父节点最后一个子节点的所有`p`节点  |\n|        `:root`         |       `:root`        |               选择文档的根节点                |\n|        `:empty`        |      `p:empty`       |  选择没有子节点的所有`p`节点（包括文本节点）  |\n|       `:target`        |    `#news:target`    |           选择当前活动的`#news`节点           |\n|       `:enabled`       |   `input:enabled`    |           选择每个启用的`input`节点           |\n|      `:disabled`       |   `input:disabled`   |           选择每个禁用的`input`节点           |\n|       `:checked`       |   `input:checked`    |          选择每个被选中的`input`节点          |\n|    `:not(selector)`    |        `:not`        |            选择非`p`节点的所有节点            |\n|     `::selection`      |    `::selection`     |           选择被用户选取的节点部分            |\n\n另外，还有一种比较常用的选择器是XPath，这种选择方式后面会详细介绍。\n\n本节介绍了网页的基本结构和节点间的关系，了解了这些内容，我们才有更加清晰的思路去解析和提取网页内容。', 0, '用浏览器访问网站时，页面各不相同，你有没有想过它为何会呈现这个样子呢？本节中，我们就来了解一下网页的基本组成、结构和节点等内容。', '网页基础', 0, '2019-05-06 11:04:10', 0, 1, 1);
INSERT INTO `article` VALUES (7, '我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。\n\n## 1. 爬虫概述\n\n简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，下面概要介绍一下。\n\n### (1) 获取网页\n\n爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。\n\n前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？总不能手工去截取网页源码吧？\n\n不用担心，Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。我们可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。\n\n### (2) 提取信息\n\n获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。\n\n另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。\n\n提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。\n\n### (3) 保存数据\n\n提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，也可保存至远程服务器，如借助SFTP进行操作等。\n\n### (4) 自动化程序\n\n说到自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。\n\n## 2. 能抓怎样的数据\n\n在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着HTML代码，而最常抓取的便是HTML源代码。\n\n另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。\n\n此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。\n\n另外，还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。\n\n上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。\n\n## 3. JavaScript渲染页面\n\n有时候，我们在用urllib或requests抓取网页时，得到的源代码实际和浏览器中看到的不一样。\n\n这是一个非常常见的问题。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳，例如：\n\n```html\n<!DOCTYPE html>\n<html>\n    \n    <head>\n        <meta charset=\"UTF-8\">\n        <title>**This** **is** a Demo</title>\n    </head>\n    \n    <body>\n        <div id=\"container\"></div>\n    </body>\n    <script src=\"app.js\"></script>\n\n</html>\n```\n\n`body`节点里面只有一个`id`为`container`的节点，但是需要注意在`body`节点后引入了app.js，它便负责整个网站的渲染。\n\n在浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的JavaScript代码，而JavaScript则会改变HTML中的节点，向其添加内容，最后得到完整的页面。\n\n但是在用urllib或requests等库请求当前页面时，我们得到的只是这个HTML代码，它不会帮助我们去继续加载这个JavaScript文件，这样也就看不到浏览器中的内容了。\n\n这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。\n\n因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。\n\n后面，我们会详细介绍如何采集JavaScript渲染的网页。\n\n本节介绍了爬虫的一些基本原理，这可以帮助我们在后面编写爬虫时更加得心应手。', 0, '我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。', '爬虫基本原理', 0, '2019-05-06 11:04:52', 0, 1, 1);
INSERT INTO `article` VALUES (8, '我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么美好，然而一杯茶的功夫可能就会出现错误，比如403 Forbidden，这时候打开网页一看，可能会看到“您的IP访问频率太高”这样的提示。出现这种现象的原因是网站采取了一些反爬虫措施。比如，服务器会检测某个IP在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封IP。\n\n既然服务器检测的是某个IP单位时间的请求次数，那么借助某种方式来伪装我们的IP，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封IP了吗？\n\n一种有效的方式就是使用代理，后面会详细说明代理的用法。在这之前，需要先了解下代理的基本原理，它是怎样实现IP伪装的呢？\n\n## 1. 基本原理\n\n代理实际上指的就是代理服务器，英文叫作proxy server，它的功能是代理网络用户去取得网络信息。形象地说，它是网络信息的中转站。在我们正常请求一个网站时，是发送了请求给Web服务器，Web服务器把响应传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向Web服务器发起请求，而是向代理服务器发出请求，请求会发送给代理服务器，然后由代理服务器再发送给Web服务器，接着由代理服务器再把Web服务器返回的响应转发给本机。这样我们同样可以正常访问网页，但这个过程中Web服务器识别出的真实IP就不再是我们本机的IP了，就成功实现了IP伪装，这就是代理的基本原理。\n\n## 2. 代理的作用\n\n那么，代理有什么作用呢？我们可以简单列举如下。\n\n- 突破自身IP访问限制，访问一些平时不能访问的站点。\n- 访问一些单位或团体内部资源：比如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类FTP下载上传，以及各类资料查询共享等服务。\n- 提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。\n- 隐藏真实IP：上网者也可以通过这种方法隐藏自己的IP，免受攻击。对于爬虫来说，我们用代理就是为了隐藏自身IP，防止自身的IP被封锁。\n\n## 3. 爬虫代理\n\n对于爬虫来说，由于爬虫爬取速度过快，在爬取过程中可能遇到同一个IP访问过于频繁的问题，此时网站就会让我们输入验证码登录或者直接封锁IP，这样会给爬取带来极大的不便。\n\n使用代理隐藏真实的IP，让服务器误以为是代理服务器在请求自己。这样在爬取过程中通过不断更换代理，就不会被封锁，可以达到很好的爬取效果。\n\n## 4. 代理分类\n\n代理分类时，既可以根据协议区分，也可以根据其匿名程度区分。\n\n### (1) 根据协议区分\n\n根据代理的协议，代理可以分为如下类别。\n\n- **FTP代理服务器**：主要用于访问FTP服务器，一般有上传、下载以及缓存功能，端口一般为21、2121等。\n- **HTTP代理服务器**：主要用于访问网页，一般有内容过滤和缓存功能，端口一般为80、8080、3128等。\n- **SSL/TLS代理**：主要用于访问加密网站，一般有SSL或TLS加密功能（最高支持128位加密强度），端口一般为443。\n- **RTSP代理**：主要用于访问Real流媒体服务器，一般有缓存功能，端口一般为554。\n- **Telnet代理**：主要用于telnet远程控制（黑客入侵计算机时常用于隐藏身份），端口一般为23。\n- **POP3/SMTP代理**：主要用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25。\n- **SOCKS代理**：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。SOCKS代理协议又分为SOCKS4和SOCKS5，前者只支持TCP，而后者支持TCP和UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCK4能做到的SOCKS5都可以做到，但SOCKS5能做到的SOCK4不一定能做到。\n\n### (2) 根据匿名程度区分\n\n根据代理的匿名程度，代理可以分为如下类别。\n\n- **高度匿名代理**：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的IP是代理服务器的IP。\n- **普通匿名代理**：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP。代理服务器通常会加入的HTTP头有`HTTP_VIA`和`HTTP_X_FORWARDED_FOR`。\n- **透明代理**：不但改动了数据包，还会告诉服务器客户端的真实IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网中的硬件防火墙。\n- **间谍代理**：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。\n\n## 5. 常见代理设置\n\n- 使用网上的免费代理：最好使用高匿代理，另外可用的代理不多，需要在使用前筛选一下可用代理，也可以进一步维护一个代理池。\n- 使用付费代理服务：互联网上存在许多代理商，可以付费使用，质量比免费代理好很多。\n- ADSL拨号：拨一次号换一次IP，稳定性高，也是一种比较有效的解决方案。\n\n在后文我们会详细介绍这几种代理的使用方式。\n\n## 6. 参考来源\n\n由于涉及一些专业名词知识，本节的部分内容参考来源如下。\n\n- 代理服务器 维基百科：[https://zh.wikipedia.org/wiki/代理服务器](http://epub.ituring.com.cn/article/edit/[https://zh.wikipedia.org/wiki/%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8)\n- 代理 百度百科：[https://baike.baidu.com/item/代理/3242667](https://baike.baidu.com/item/%E4%BB%A3%E7%90%86/3242667)', 0, '我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么美好，然而一杯茶的功夫可能就会出现错误，比如403 Forbidden，这时候打开网页一看，可能会看到“您的IP访问频率太高”这样的提示。出现这种现象的原因是网站采取了一些反爬虫措施。比如，服务器会检测某个IP在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封IP。', '代理基本原理', 0, '2019-05-06 11:05:50', 0, 1, 1);
INSERT INTO `article` VALUES (9, '- ## 1. 准备工作\n\n  在开始之前，请确保已经正确安装好了requests库。\n\n  ## 2. 实例引入\n\n  urllib库中的`urlopen()`方法实际上是以GET方式请求网页，而requests中相应的方法就是`get()`方法，是不是感觉表达更明确一些？下面通过实例来看一下：\n\n  ```python\n  import requests r = requests.get(\'https://www.baidu.com/\')print(type(r))print(r.status_code)print(type(r.text))print(r.text)print(r.cookies)\n  ```\n\n  运行结果如下：\n\n  ```html\n  <html>\n      <head>\n          <script>\n              location.replace(location.href.replace(\"https://\", \"http://\"));\n          </script>\n      </head>\n      <body>\n          <noscript>\n              <meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\">\n          </noscript>\n      </body>\n  </html>\n  <RequestsCookieJar[<Cookie BIDUPSID=992C3B26F4C4D09505C5E959D5FBC005 for .baidu.com/>,\n  <Cookie PSTM=1472227535 for .baidu.com/>,\n  <Cookie __bsi=15304754498609545148_00_40_N_N_2_0303_C02F_N_N_N_0 for .www.baidu.com/>,\n  <Cookie BD_NOT_HTTPS=1 for www.baidu.com/>]>\n  ```\n\n  这里我们调用`get()`方法实现与`urlopen()`相同的操作，得到一个`Response`对象，然后分别输出了`Response`的类型、状态码、响应体的类型、内容以及Cookies。\n\n  通过运行结果可以发现，它的返回类型是`requests.models.Response`，响应体的类型是字符串`str`，Cookies的类型是`RequestsCookieJar`。\n\n  使用`get()`方法成功实现一个GET请求，这倒不算什么，更方便之处在于其他的请求类型依然可以用一句话来完成，示例如下：\n\n  ```python\n  r = requests.post(\'http://httpbin.org/post\')\n  r = requests.put(\'http://httpbin.org/put\')\n  r = requests.delete(\'http://httpbin.org/delete\')\n  r = requests.head(\'http://httpbin.org/get\')\n  r = requests.options(\'http://httpbin.org/get\')\n  ```\n\n  这里分别用`post()`、`put()`、`delete()`等方法实现了POST、PUT、DELETE等请求。是不是比urllib简单太多了？\n\n  其实这只是冰山一角，更多的还在后面。\n\n  ## 3. GET请求\n\n  HTTP中最常见的请求之一就是GET请求，下面首先来详细了解一下利用requests构建GET请求的方法。\n\n  ### 基本实例\n\n  首先，构建一个最简单的GET请求，请求的链接为[http://httpbin.org/get](http://epub.ituring.com.cn/article/edit/[http://httpbin.org/get)，该网站会判断如果客户端发起的是GET请求的话，它返回相应的请求信息：\n\n  ```python\n  import requests r = requests.get(\'http://httpbin.org/get\')print(r.text)\n  ```\n\n  运行结果如下：\n\n  ```json\n  {\n      \"args\": {},\n      \"headers\": {\n          \"Accept\": \"*/*\",\n          \"Accept-Encoding\": \"gzip, deflate\",\n          \"Host\": \"httpbin.org\",\n          \"User-Agent\": \"python-requests/2.10.0\"\n      },\n      \"origin\": \"122.4.215.33\",\n      \"url\": \"http://httpbin.org/get\"\n  }\n  ```\n\n  可以发现，我们成功发起了GET请求，返回结果中包含请求头、URL、IP等信息。\n\n  那么，对于GET请求，如果要附加额外的信息，一般怎样添加呢？比如现在想添加两个参数，其中`name`是`germey`，`age`是22。要构造这个请求链接，是不是要直接写成：\n\n  ```python\n  r = requests.get(\'http://httpbin.org/get?name=germey&age=22\')\n  ```\n\n  这样也可以，但是是不是有点不人性化呢？一般情况下，这种信息数据会用字典来存储。那么，怎样来构造这个链接呢？\n\n  这同样很简单，利用`params`这个参数就好了，示例如下：\n\n  ```python\n  import requests \n  data = {    \'name\': \'germey\',    \'age\': 22}\n  r = requests.get(\"http://httpbin.org/get\", params=data)\n  print(r.text)\n  ```\n\n  运行结果如下：\n\n  ```json\n  {\n      \"args\": {\n          \"age\": \"22\",\n          \"name\": \"germey\"\n      },\n      \"headers\": {\n          \"Accept\": \"*/*\",\n          \"Accept-Encoding\": \"gzip, deflate\",\n          \"Host\": \"httpbin.org\",\n          \"User-Agent\": \"python-requests/2.10.0\"\n      },\n      \"origin\": \"122.4.215.33\",\n      \"url\": \"http://httpbin.org/get?age=22&name=germey\"\n  }\n  ```\n\n  通过运行结果可以判断，请求的链接自动被构造成了：<http://httpbin.org/get?age=22&name=germey>。\n\n  另外，网页的返回类型实际上是`str`类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个字典格式的话，可以直接调用`json()`方法。示例如下：\n\n  ```python\n  import requests \n  r = requests.get(\"http://httpbin.org/get\")\n  print(type(r.text))\n  print(r.json())\n  print(type(r.json()))\n  ```\n\n  运行结果如下：\n\n  ```json\n  {\n      \'headers\': {\n          \'Accept-Encoding\': \'gzip, deflate\',\n          \'Accept\': \'*/*\',\n          \'Host\': \'httpbin.org\',\n          \'User-Agent\': \'python-requests/2.10.0\'\n      },\n      \'url\': \'http://httpbin.org/get\',\n      \'args\': {},\n      \'origin\': \'182.33.248.131\'\n  }\n  ```\n\n  可以发现，调用`json()`方法，就可以将返回结果是JSON格式的字符串转化为字典。\n\n  但需要注意的书，如果返回结果不是JSON格式，便会出现解析错误，抛出`json.decoder.JSONDecodeError`异常。\n\n  ### 抓取网页\n\n  上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容了。下面以“知乎”→“发现”页面为例来看一下：\n\n  ```python\n  import requests\n  import re \n  headers = {    \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'}\n  r = requests.get(\"https://www.zhihu.com/explore\", headers=headers)\n  pattern = re.compile(\'explore-feed.*?question_link.*?>(.*?)</a>\', re.S)\n  titles = re.findall(pattern, r.text)\n  print(titles)\n  ```\n\n  这里我们加入了`headers`信息，其中包含了`User-Agent`字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取。\n\n  接下来我们用到了最基础的正则表达式来匹配出所有的问题内容。关于正则表达式的相关内容，我们会在3.3节中详细介绍，这里作为实例来配合讲解。\n\n  运行结果如下：\n\n  ```json\n  [\'\\n为什么很多人喜欢提及「拉丁语系」这个词？\\n\', \'\\n在没有水的情况下水系宝可梦如何战斗？\\n\', \'\\n有哪些经验可以送给 Kindle 新人？\\n\', \'\\n谷歌的广告业务是如何赚钱的？\\n\',\n      \'\\n程序员该学习什么，能在上学期间挣钱？\\n\', \'\\n有哪些原本只是一个小消息，但回看发现是个惊天大新闻的例子？\\n\', \'\\n如何评价今敏？\\n\', \'\\n源氏是怎么把那么长的刀从背后拔出来的？\\n\',\n      \'\\n年轻时得了绝症或大病是怎样的感受？\\n\', \'\\n年轻时得了绝症或大病是怎样的感受？\\n\']\n  ```\n\n  我们发现，这里成功提取出了所有的问题内容。\n\n  ### 抓取二进制数据\n\n  在上面的例子中，我们抓取的是知乎的一个页面，实际上它返回的是一个HTML文档。如果想抓去图片、音频、视频等文件，应该怎么办呢？\n\n  图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制码。\n\n  下面以GitHub的站点图标为例来看一下：\n\n  ```python\n  import requests \n  r = requests.get(\"https://github.com/favicon.ico\")\n  print(r.text)\n  print(r.content)\n  ```\n\n  这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标，如图3-3所示。\n\n  ![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-3.png)图3-3 站点图标\n\n  这里打印了`Response`对象的两个属性，一个是`text`，另一个是`content`。\n\n  运行结果如图3-4所示，其中前两行是`r.text`的结果，最后一行是`r.content`的结果。\n\n  ![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-4.png)图3-4 运行结果\n\n  可以注意到，前者出现了乱码，后者结果前带有一个`b`，这代表是`bytes`类型的数据。由于图片是二进制数据，所以前者在打印时转化为`str`类型，也就是图片直接转化为字符串，这理所当然会出现乱码。\n\n  接着，我们将刚才提取到的图片保存下来：\n\n  ```python\n  import requests\n  r = requests.get(\"https://github.com/favicon.ico\")\n  with open(\'favicon.ico\', \'wb\') as f:\n      f.write(r.content)\n  ```\n\n  这里用了`open()`方法，它的第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据。\n\n  运行结束之后，可以发现在文件夹中出现了名为favicon.ico的图标，如图3-5所示。\n\n  [![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-5.ico)](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-5.ico)图3-5 图标\n\n  同样地，音频和视频文件也可以用这种方法获取。\n\n  ### 添加`headers`\n\n  与`urllib.request`一样，我们也可以通过`headers`参数来传递头信息。\n\n  比如，在上面“知乎”的例子中，如果不传递`headers`，就不能正常请求：\n\n  ```python\n  import requests \n  r = requests.get(\"https://www.zhihu.com/explore\")\n  print(r.text)\n  ```\n\n  运行结果如下：\n\n  ```html\n  <html>\n      <body>\n          <h1>500 Server Error</h1>An internal server error occured.</body>\n  </html>\n  ```\n\n  但如果加上`headers`并加上`User-Agent`信息，那就没问题了：\n\n  ```python\n  import requests \n  headers = {    \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\'}\n  r = requests.get(\"https://www.zhihu.com/explore\", headers=headers)\n  print(r.text)\n  ```\n\n  当然，我们可以在`headers`这个参数中任意添加其他的字段信息。\n\n  ## 4. POST请求\n\n  前面我们了解了最基本的GET请求，另外一种比较常见的请求方式是POST。使用`requests`实现POST请求同样非常简单，示例如下：\n\n  ```python\n  import requests\n  data = {\'name\': \'germey\', \'age\': \'22\'}\n  r = requests.post(\"http://httpbin.org/post\", data=data)\n  print(r.text)\n  ```\n\n  这里还是请求<http://httpbin.org/post>，该网站可以判断如果请求是POST方式，就把相关请求信息返回。\n\n  运行结果如下：\n\n  ```json\n  {  \"args\": {},   \"data\": \"\",   \"files\": {},   \"form\": {    \"age\": \"22\",     \"name\": \"germey\"  },   \"headers\": {    \"Accept\": \"*/*\",     \"Accept-Encoding\": \"gzip, deflate\",     \"Content-Length\": \"18\",     \"Content-Type\": \"application/x-www-form-urlencoded\",     \"Host\": \"httpbin.org\",     \"User-Agent\": \"python-requests/2.10.0\"  },   \"json\": null,   \"origin\": \"182.33.248.131\",   \"url\": \"http://httpbin.org/post\"}\n  ```\n\n  可以发现，我们成功获得了返回结果，其中`form`部分就是提交的数据，这就证明POST请求成功发送了。\n\n  ## 5. 响应\n\n  发送请求后，得到的自然就是响应。在上面的实例中，我们使用`text`和`content`获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies等。示例如下：\n\n  ```python\n  import requests \n  r = requests.get(\'http://www.jianshu.com\')\n  print(type(r.status_code), r.status_code)\n  print(type(r.headers), r.headers)\n  print(type(r.cookies), r.cookies)\n  print(type(r.url), r.url)\n  print(type(r.history), r.history)\n  ```\n\n  这里分别打印输出`status_code`属性得到状态码，输出`headers`属性得到响应头，输出`cookies`属性得到Cookies，输出`url`属性得到URL，输出`history`属性得到请求历史。\n\n  运行结果如下：\n\n  ```html\n  {\'X-Runtime\': \'0.006363\', \'Connection\': \'keep-alive\', \'Content-Type\': \'text/html; charset=utf-8\', \'X-Content-Type-Options\': \'nosniff\', \'Date\': \'Sat, 27 Aug 2016 17:18:51 GMT\', \'Server\': \'nginx\', \'X-Frame-Options\': \'DENY\', \'Content-Encoding\': \'gzip\',\n  \'Vary\': \'Accept-Encoding\', \'ETag\': \'W/\"3abda885e0e123bfde06d9b61e696159\"\', \'X-XSS-Protection\': \'1; mode=block\', \'X-Request-Id\': \'a8a3c4d5-f660-422f-8df9-49719dd9b5d4\', \'Transfer-Encoding\': \'chunked\', \'Set-Cookie\': \'read_mode=day; path=/, default_font=font2;\n  path=/, _session_id=xxx; path=/; HttpOnly\', \'Cache-Control\': \'max-age=0, private, must-revalidate\'}\n  <class \'requests.cookies.RequestsCookieJar\'>\n      <RequestsCookieJar[<Cookie _session_id=xxx for www.jianshu.com/>,\n      <Cookie default_font=font2 for www.jianshu.com/>,\n      <Cookie read_mode=day for www.jianshu.com/>]>\n  ```\n\n  因为`session_id`过长，在此简写。可以看到，`headers`和`cookies`这两个属性得到的结果分别是`CaseInsensitiveDict`和`RequestsCookieJar`类型。\n\n  状态码常用来判断请求是否成功，而requests还提供了一个内置的状态码查询对象`requests.codes`，示例如下：\n\n  ```python\n  import requests \n  r = requests.get(\'http://www.jianshu.com\')\n  exit() \n  if not r.status_code == requests.codes.ok \n  else\n  print(\'Request Successfully\')\n  ```\n\n  这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用`requests.codes.ok`得到的是成功的状态码200。\n\n  那么，肯定不能只有`ok`这个条件码。下面列出了返回码和相应的查询条件：\n\n  ```\n  # 信息性状态码\n  100: (\'continue\',),\n  101: (\'switching_protocols\',),\n  102: (\'processing\',),\n  103: (\'checkpoint\',),\n  122: (\'uri_too_long\', \'request_uri_too_long\'), \n  # 成功状态码\n  200: (\'ok\', \'okay\', \'all_ok\', \'all_okay\', \'all_good\', \'\\\\o/\', \'✓\'),\n  201: (\'created\',),\n  202: (\'accepted\',),\n  203: (\'non_authoritative_info\', \'non_authoritative_information\'),\n  204: (\'no_content\',),\n  205: (\'reset_content\', \'reset\'),\n  206: (\'partial_content\', \'partial\'),\n  207: (\'multi_status\', \'multiple_status\', \'multi_stati\', \'multiple_stati\'),\n  208: (\'already_reported\',),\n  226: (\'im_used\',), \n  # 重定向状态码\n  300: (\'multiple_choices\',),\n  301: (\'moved_permanently\', \'moved\', \'\\\\o-\'),\n  302: (\'found\',),\n  303: (\'see_other\', \'other\'),\n  304: (\'not_modified\',),\n  305: (\'use_proxy\',),\n  306: (\'switch_proxy\',),\n  307: (\'temporary_redirect\', \'temporary_moved\', \'temporary\'),\n  308: (\'permanent_redirect\',      \'resume_incomplete\', \'resume\',), \n  # These 2 to be removed in 3.0 \n  # 客户端错误状态码\n  400: (\'bad_request\', \'bad\'),\n  401: (\'unauthorized\',),\n  402: (\'payment_required\', \'payment\'),\n  403: (\'forbidden\',),\n  404: (\'not_found\', \'-o-\'),\n  405: (\'method_not_allowed\', \'not_allowed\'),\n  406: (\'not_acceptable\',),\n  407: (\'proxy_authentication_required\', \'proxy_auth\', \'proxy_authentication\'),\n  408: (\'request_timeout\', \'timeout\'),\n  409: (\'conflict\',),\n  410: (\'gone\',),\n  411: (\'length_required\',),\n  412: (\'precondition_failed\', \'precondition\'),\n  413: (\'request_entity_too_large\',),\n  414: (\'request_uri_too_large\',),\n  415: (\'unsupported_media_type\', \'unsupported_media\', \'media_type\'),\n  416: (\'requested_range_not_satisfiable\', \'requested_range\', \'range_not_satisfiable\'),\n  417: (\'expectation_failed\',),\n  418: (\'im_a_teapot\', \'teapot\', \'i_am_a_teapot\'),\n  421: (\'misdirected_request\',),\n  422: (\'unprocessable_entity\', \'unprocessable\'),\n  423: (\'locked\',),\n  424: (\'failed_dependency\', \'dependency\'),\n  425: (\'unordered_collection\', \'unordered\'),\n  426: (\'upgrade_required\', \'upgrade\'),\n  428: (\'precondition_required\', \'precondition\'),\n  429: (\'too_many_requests\', \'too_many\'),\n  431: (\'header_fields_too_large\', \'fields_too_large\'),\n  444: (\'no_response\', \'none\'),\n  449: (\'retry_with\', \'retry\'),\n  450: (\'blocked_by_windows_parental_controls\', \'parental_controls\'),\n  451: (\'unavailable_for_legal_reasons\', \'legal_reasons\'),\n  499: (\'client_closed_request\',),\n  # 服务端错误状态码\n  500: (\'internal_server_error\', \'server_error\', \'/o\\\\\', \'✗\'),\n  501: (\'not_implemented\',),\n  502: (\'bad_gateway\',),\n  503: (\'service_unavailable\', \'unavailable\'),\n  504: (\'gateway_timeout\',),\n  505: (\'http_version_not_supported\', \'http_version\'),\n  506: (\'variant_also_negotiates\',),\n  507: (\'insufficient_storage\',),\n  509: (\'bandwidth_limit_exceeded\', \'bandwidth\'),\n  510: (\'not_extended\',),\n  511: (\'network_authentication_required\', \'network_auth\', \'network_authentication\')\n  ```\n\n  比如，如果想判断结果是不是404状态，可以用`requests.codes.not_found`来比对。', 0, '本节讲解Requests库的基本用法', 'Requests库的基本用法', 0, '2019-05-06 11:06:48', 0, 1, 1);
INSERT INTO `article` VALUES (10, '在前一节中，我们了解了requests的基本用法，如基本的GET、POST请求以及`Response`对象。本节中，我们再来了解下requests的一些高级用法，如文件上传、cookie设置、代理设置等。\n\n## 1. 文件上传\n\n我们知道requests可以模拟提交一些数据。假如有的网站需要上传文件，我们也可以用它来实现，这非常简单，示例如下：\n\n```python\nimport requests \nfiles = {\'file\': open(\'favicon.ico\', \'rb\')}\nr = requests.post(\"http://httpbin.org/post\", files=files)\nprint(r.text)\n```\n\n在前一节中我们保存了一个文件favicon.ico，这次用它来模拟文件上传的过程。需要注意的是，favicon.ico需要和当前脚本在同一目录下。如果有其他文件，当然也可以使用其他文件来上传，更改下代码即可。\n\n运行结果如下：\n\n```json\n{\n    \"args\": {},\n    \"data\": \"\",\n    \"files\": {\n        \"file\": \"data:application/octet-stream;base64,AAAAAA...=\"\n    },\n    \"form\": {},\n    \"headers\": {\n        \"Accept\": \"*/*\",\n        \"Accept-Encoding\": \"gzip, deflate\",\n        \"Content-Length\": \"6665\",\n        \"Content-Type\": \"multipart/form-data; boundary=809f80b1a2974132b133ade1a8e8e058\",\n        \"Host\": \"httpbin.org\",\n        \"User-Agent\": \"python-requests/2.10.0\"\n    },\n    \"json\": ** null ** ,\n    \"origin\": \"60.207.237.16\",\n    \"url\": \"http://httpbin.org/post\"\n}\n```\n\n以上省略部分内容，这个网站会返回响应，里面包含`files`这个字段，而`form`字段是空的，这证明文件上传部分会单独有一个`files`字段来标识。\n\n## 2. Cookies\n\n前面我们使用urllib处理过Cookies，写法比较复杂，而有了requests，获取和设置Cookies只需一步即可完成。\n\n我们先用一个实例看一下获取Cookies的过程：\n\n```python\nimport requests \nr = requests.get(\"https://www.baidu.com\")\nprint(r.cookies)\nfor key, value in r.cookies.items():\n    print(key + \'=\' + value)\n```\n\n运行结果如下：\n\n```html\n<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>,\n<Cookie __bsi=13533594356813414194_00_14_N_N_2_0303_C02F_N_N_N_0 for .www.baidu.com/>]>BDORZ=27315__bsi=13533594356813414194_00_14_N_N_2_0303_C02F_N_N_N_0\n```\n\n这里我们首先调用`cookies`属性即可成功得到Cookies，可以发现它是`RequestCookieJar`类型。然后用`items()`方法将其转化为元组组成的列表，遍历输出每一个Cookie的名称和值，实现Cookie的遍历解析。\n\n当然，我们也可以直接用Cookie来维持登录状态，下面以知乎为例来说明。首先登录知乎，将`Headers`中的`Cookie`内容复制下来，如图3-6所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-6.png)图3-6 `Cookie`\n\n这里可以替换成你自己的`Cookie`，将其设置到`Headers`里面，然后发送请求，示例如下：\n\n```python\nimport requests \nheaders = {    \'Cookie\': \'q_c1=31653b264a074fc9a57816d1ea93ed8b\\|1474273938000\\|1474273938000; d_c0=\"AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=\\|1474273938\"; __utmv=51854390.100-1\\|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t=\"2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==\";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn\\|1474887858\\|64b4d4234a21de774c42c837fe0b672fdb5763b0\',    \'Host\': \'www.zhihu.com\',    \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36\',}\nr = requests.get(\'https://www.zhihu.com\', headers=headers)\nprint(r.text)\n```\n\n我们发现，结果中包含了登录后的结果，如图3-7所示，这证明登录成功。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-7.jpg)图3-7 运行结果\n\n当然，你也可以通过`cookies`参数来设置，不过这样就需要构造`RequestsCookieJar`对象，而且需要分割一下`cookies`。这相对烦琐，不过效果是相同的，示例如下：\n\n```python\nimport requests \ncookies = \'q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0=\"AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938\"; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t=\"2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==\";z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0\'\njar = requests.cookies.RequestsCookieJar()\nheaders = {    \'Host\': \'www.zhihu.com\',    \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36\'}\nfor cookie in cookies.split(\';\'):\n    key, value = cookie.split(\'=\', 1)\n    jar.set(key, value)\nr = requests.get(\"http://www.zhihu.com\", cookies=jar, headers=headers)\nprint(r.text)\n```\n\n这里我们首先新建了一个`RequestCookieJar`对象，然后将复制下来的`cookies`利用`split()`方法分割，接着利用`set()`方法设置好每个Cookie的`key`和`value`，然后通过调用requests的`get()`方法并传递给`cookies`参数即可。当然，由于知乎本身的限制，`headers`参数也不能少，只不过不需要在原来的`headers`参数里面设置`cookie`字段了。\n\n测试后，发现同样可以正常登录知乎。\n\n## 3. 会话维持\n\n在requests中，如果直接利用`get()`或`post()`等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的会话，也就是说相当于你用了两个浏览器打开了不同的页面。\n\n设想这样一个场景，第一个请求利用`post()`方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，你又用了一次`get()`方法去请求个人信息页面。实际上，这相当于打开了两个浏览器，是两个完全不相关的会话，能成功获取个人信息吗？那当然不能。\n\n有小伙伴可能说了，我在两次请求时设置一样的`cookies`不就行了？可以，但这样做起来显得很烦琐，我们有更简单的解决方法。\n\n其实解决这个问题的主要方法就是维持同一个会话，也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但是我又不想每次设置`cookies`，那该怎么办呢？这时候就有了新的利器——`Session`对象。\n\n利用它，我们可以方便地维护一个会话，而且不用担心`cookies`的问题，它会帮我们自动处理好。示例如下：\n\n```python\nimport requests\nrequests.get(\'http://httpbin.org/cookies/set/number/123456789\')\nr = requests.get(\'http://httpbin.org/cookies\')\nprint(r.text)\n```\n\n这里我们请求了一个测试网址<http://httpbin.org/cookies/set/number/123456789>。请求这个网址时，可以设置一个cookie，名称叫作number，内容是123456789，随后又请求了<http://httpbin.org/cookies>，此网址可以获取当前的Cookies。\n\n这样能成功获取到设置的Cookies吗？试试看。\n\n运行结果如下：\n\n```json\n{  \"cookies\": {}}\n```\n\n这并不行。我们再用`Session`试试看：\n\n```python\nimport requests \ns = requests.Session()\ns.get(\'http://httpbin.org/cookies/set/number/123456789\')\nr = s.get(\'http://httpbin.org/cookies\')\nprint(r.text)\n```\n\n再看下运行结果：\n\n```json\n{  \"cookies\": {    \"number\": \"123456789\"  }}\n```\n\n成功获取！这下能体会到同一个会话和不同会话的区别了吧！\n\n所以，利用`Session`，可以做到模拟同一个会话而不用担心Cookies的问题。它通常用于模拟登录成功之后再进行下一步的操作。\n\n`Session`在平常用得非常广泛，可以用于模拟在一个浏览器中打开同一站点的不同页面，后面会有专门的章节来讲解这部分内容。\n\n## 4. SSL证书验证\n\n此外，requests还提供了证书验证的功能。当发送HTTP请求的时候，它会检查SSL证书，我们可以使用`verify`参数控制是否检查此证书。其实如果不加`verify`参数的话，默认是`True`，会自动验证。\n\n前面我们提到过，12306的证书没有被官方CA机构信任，会出现证书验证错误的结果。我们现在访问它，都可以看到一个证书问题的页面，如图3-8所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-8.png)图3-8 错误页面\n\n现在我们用requests来测试一下：\n\n```python\nimport requests \nresponse = requests.get(\'https://www.12306.cn\')\nprint(response.status_code)\n```\n\n运行结果如下：\n\n```python\nrequests.exceptions.SSLError: (\"bad handshake: Error([(\'SSL routines\', \'tls_process_server_certificate\', \'certificate verify failed\')],)\",)\n```\n\n这里提示一个错误`SSLError`，表示证书验证错误。所以，如果请求一个HTTPS站点，但是证书验证错误的页面时，就会报这样的错误，那么如何避免这个错误呢？很简单，把`verify`参数设置为`False`即可。相关代码如下：\n\n```python\nimport requests \nresponse = requests.get(\'https://www.12306.cn\', verify=False)\nprint(response.status_code)\n```\n\n这样就会打印出请求成功的状态码：\n\n```\n/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request **is** being made. Adding certificate verification **is** strongly advised. See: https:*//urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings*  InsecureRequestWarning)200\n```\n\n不过我们发现报了一个警告，它建议我们给它指定证书。我们可以通过设置忽略警告的方式来屏蔽这个警告：\n\n```python\nimport requestsfrom \nrequests.packages \nimport urllib3 \nurllib3.disable_warnings()\nresponse = requests.get(\'https://www.12306.cn\', verify=**False**)\nprint(response.status_code)\n```\n\n或者通过捕获警告到日志的方式忽略警告：\n\n```python\nimport loggingimport \nrequestslogging.captureWarnings(True)\nresponse = requests.get(\'https://www.12306.cn\', verify=False)\nprint(response.status_code)\n```\n\n当然，我们也可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组：\n\n```python\nimport requests\nresponse = requests.get(\'https://www.12306.cn\', cert=(\'/path/server.crt\', \'/path/key\'))\nprint(response.status_code)\n```\n\n当然，上面的代码是演示实例，我们需要有crt和key文件，并且指定它们的路径。注意，本地私有证书的`key`必须是解密状态，加密状态的`key`是不支持的。\n\n## 5. 代理设置\n\n对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的IP，导致一定时间段内无法访问。\n\n那么，为了防止这种情况发生，我们需要设置代理来解决这个问题，这就需要用到`proxies`参数。可以用这样的方式设置：\n\n```python\nimport requests \nproxies = {  \"http\": \"http://10.10.1.10:3128\",  \"https\": \"http://10.10.1.10:1080\",} \nrequests.get(\"https://www.taobao.com\", proxies=proxies)\n```\n\n当然，直接运行这个实例可能不行，因为这个代理可能是无效的，请换成自己的有效代理试验一下。\n\n若代理需要使用HTTP Basic Auth，可以使用类似http://user:password@host:port这样的语法来设置代理，示例如下：\n\n```python\nimport requests \nproxies = {    \"http\": \"http://user:password@10.10.1.10:3128/\",}\nrequests.get(\"https://www.taobao.com\", proxies=proxies)\n```\n\n除了基本的HTTP代理外，requests还支持SOCKS协议的代理。\n\n首先，需要安装socks这个库：\n\n```sh\npip3 install \'requests[socks]\'\n```\n\n然后就可以使用SOCKS协议代理了，示例如下：\n\n```python\nimport requests \nproxies = {    \'http\': \'socks5://user:password@host:port\',    \'https\': \'socks5://user:password@host:port\'}\nrequests.get(\"https://www.taobao.com\", proxies=proxies)\n```\n\n## 6. 超时设置\n\n在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能收到响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。这需要用到`timeout`参数。这个时间的计算是发出请求到服务器返回响应的时间。示例如下：\n\n```python\nimport requests \nr = requests.get(\"https://www.taobao.com\", timeout = 1)\nprint(r.status_code)\n```\n\n通过这样的方式，我们可以将超时时间设置为1秒，如果1秒内没有响应，那就抛出异常。\n\n实际上，请求分为两个阶段，即连接（connect）和读取（read）。\n\n上面设置的`timeout`将用作连接和读取这二者的`timeout`总和。\n\n如果要分别指定，就可以传入一个元组：\n\n```python\nr = requests.get(\'https://www.taobao.com\', timeout=(5,11, 30))\n```\n\n如果想永久等待，可以直接将`timeout`设置为`None`，或者不设置直接留空，因为默认是`None`。这样的话，如果服务器还在运行，但是响应特别慢，那就慢慢等吧，它永远不会返回超时错误的。其用法如下：\n\n```python\nr = requests.get(\'https://www.taobao.com\', timeout=None)\n```\n\n或直接不加参数：\n\n```python\nr = requests.get(\'https://www.taobao.com\')\n```\n\n## 7. 身份认证\n\n在访问网站时，我们可能会遇到这样的认证页面，如图3-9所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-9.jpg)图3-9 认证页面\n\n此时可以使用requests自带的身份认证功能，示例如下：\n\n```python\nimport requests from requests.auth \nimport HTTPBasicAuth \nr = requests.get(\'http://localhost:5000\', auth=HTTPBasicAuth(\'username\', \'password\'))\nprint(r.status_code)\n```\n\n如果用户名和密码正确的话，请求时就会自动认证成功，会返回200状态码，如果认证失败，则返回401状态码。\n\n当然，如果参数都传一个`HTTPBasicAuth`类，就显得有点烦琐了，所以requests提供了一个更简单的写法，可以直接传一个元组，它会默认使用`HTTPBasicAuth`这个类来认证。\n\n所以上面的代码可以直接简写如下：\n\n```python\nimport requests \nr = requests.get(\'http://localhost:5000\', auth=(\'username\', \'password\'))\nprint(r.status_code)\n```\n\n此外，requests还提供了其他认证方式，如OAuth认证，不过此时需要安装oauth包，安装命令如下：\n\n```sh\npip3 install requests_oauthlib\n```\n\n使用OAuth1认证的方法如下：\n\n```python\nimport requests from requests_oauthlib \nimport OAuth1 \nurl = \'https://api.twitter.com/1.1/account/verify_credentials.json\'\nauth = OAuth1(\'YOUR_APP_KEY\', \'YOUR_APP_SECRET\',              \'USER_OAUTH_TOKEN\', \'USER_OAUTH_TOKEN_SECRET\')\nrequests.get(url, auth=auth)\n```\n\n更多详细的功能可以参考requests_oauthlib的官方文档<https://requests-oauthlib.readthedocs.org/>，在此不再赘述了。\n\n## 8. Prepared Request\n\n前面介绍urllib时，我们可以将请求表示为数据结构，其中各个参数都可以通过一个`Request`对象来表示。这在requests里同样可以做到，这个数据结构就叫Prepared Request。我们用实例看一下：\n\n```python\nfrom requests import Request, Session \nurl = \'http://httpbin.org/post\'\ndata = {    \'name\': \'germey\'}\nheaders = {    \'User-Agent\': \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36\'}\ns = Session()\nreq = Request(\'POST\', url, data=data, headers=headers)\nprepped = s.prepare_request(req)\nr = s.send(prepped)\nprint(r.text)\n```\n\n这里我们引入了`Request`，然后用`url`、`data`和`headers`参数构造了一个`Request`对象，这时需要再调用`Session`的`prepare_request()`方法将其转换为一个Prepared Request对象，然后调用`send()`方法发送即可，运行结果如下：\n\n```json\n{  \"args\": {},   \"data\": \"\",   \"files\": {},   \"form\": {    \"name\": \"germey\"  },   \"headers\": {    \"Accept\": \"*/*\",     \"Accept-Encoding\": \"gzip, deflate\",     \"Connection\": \"close\",     \"Content-Length\": \"11\",     \"Content-Type\": \"application/x-www-form-urlencoded\",     \"Host\": \"httpbin.org\",     \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36\"  },   \"json\": **null**,   \"origin\": \"182.32.203.166\",   \"url\": \"http://httpbin.org/post\"}\n```\n\n可以看到，我们达到了同样的POST请求效果。\n\n有了`Request`这个对象，就可以将请求当作独立的对象来看待，这样在进行队列调度时会非常方便。后面我们会用它来构造一个`Request`队列。\n\n', 0, '在前一节中，我们了解了requests的基本用法，如基本的GET、POST请求以及`Response`对象。本节中，我们再来了解下requests的一些高级用法，如文件上传、cookie设置、代理设置等。', 'Requests库的高级用法', 0, '2019-05-06 11:07:16', 0, 1, 1);
INSERT INTO `article` VALUES (11, '本节中，我们看一下正则表达式的相关用法。正则表达式是处理字符串的强大工具，它有自己特定的语法结构，有了它，实现字符串的检索、替换、匹配验证都不在话下。\n\n当然，对于爬虫来说，有了它，从HTML里提取想要的信息就非常方便了。\n\n## 1. 实例引入\n\n说了这么多，可能我们对它到底是个什么还是比较模糊，下面就用几个实例来看一下正则表达式的用法。\n\n打开开源中国提供的正则表达式测试工具<http://tool.oschina.net/regex/>，输入待匹配的文本，然后选择常用的正则表达式，就可以得出相应的匹配结果了。例如，这里输入待匹配的文本如下：\n\n```txt\nHello, my phone number is 010-86432100 and email is cqc@cuiqingcai.com, and my website is http://cuiqingcai.com.\n```\n\n这段字符串中包含了一个电话号码和一个电子邮件，接下来就尝试用正则表达式提取出来，如图3-10所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-10.jpg)图3-10 运行页面\n\n在网页右侧选择“匹配Email地址”，就可以看到下方出现了文本中的E-mail。如果选择“匹配网址URL”，就可以看到下方出现了文本中的URL。是不是非常神奇？\n\n其实，这里就是用了正则表达式匹配，也就是用一定的规则将特定的文本提取出来。比如，电子邮件开头是一段字符串，然后是一个@符号，最后是某个域名，这是有特定的组成格式的。另外，对于URL，开头是协议类型，然后是冒号加双斜线，最后是域名加路径。\n\n对于URL来说，可以用下面的正则表达式匹配：\n\n```regexp\n[a-zA-z]+://[^\\s]\n```\n\n用这个正则表达式去匹配一个字符串，如果这个字符串中包含类似URL的文本，那就会被提取出来。\n\n这个正则表达式看上去是乱糟糟的一团，其实不然，这里面都是有特定的语法规则的。比如，`a-z`代表匹配任意的小写字母，`\\s`表示匹配任意的空白字符，`*`就代表匹配前面的字符任意多个，这一长串的正则表达式就是这么多匹配规则的组合。\n\n写好正则表达式后，就可以拿它去一个长字符串里匹配查找了。不论这个字符串里面有什么，只要符合我们写的规则，统统可以找出来。对于网页来说，如果想找出网页源代码里有多少URL，用匹配URL的正则表达式去匹配即可。\n\n上面我们说了几个匹配规则，表3-2列出了常用的匹配规则。\n\n表3-2 常用的匹配规则\n\n|   模式   |                             描述                             |\n| :------: | :----------------------------------------------------------: |\n|   `\\w`   |                    匹配字母、数字及下划线                    |\n|   `\\W`   |               匹配不是字母、数字及下划线的字符               |\n|   `\\s`   |             匹配任意空白字符，等价于`[\\t\\n\\r\\f]`             |\n|   `\\S`   |                       匹配任意非空字符                       |\n|   `\\d`   |                 匹配任意数字，等价于`[0-9]`                  |\n|   `\\D`   |                     匹配任意非数字的字符                     |\n|   `\\A`   |                        匹配字符串开头                        |\n|   `\\Z`   |   匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串   |\n|   `\\z`   |       匹配字符串结尾，如果存在换行，同时还会匹配换行符       |\n|   `\\G`   |                    匹配最后匹配完成的位置                    |\n|   `\\n`   |                        匹配一个换行符                        |\n|   `\\t`   |                        匹配一个制表符                        |\n|   `^`    |                     匹配一行字符串的开头                     |\n|   `$`    |                     匹配一行字符串的结尾                     |\n|   `.`    | 匹配任意字符，除了换行符，当`re.DOTALL`标记被指定时，则可以匹配包括换行符的任意字符 |\n| `[...]`  |   用来表示一组字符，单独列出，比如`[amk]`匹配`a`、`m`或`k`   |\n| `[^...]` | 不在`[]`中的字符，比如`[^abc]`匹配除了`a`、`b`、`c`之外的字符 |\n|   `*`    |                     匹配0个或多个表达式                      |\n|   `+`    |                     匹配1个或多个表达式                      |\n|   `?`    |      匹配0个或1个前面的正则表达式定义的片段，非贪婪方式      |\n|  `{n}`   |                  精确匹配`n`个前面的表达式                   |\n| `{n, m}` |      匹配`n`到`m`次由前面正则表达式定义的片段，贪婪方式      |\n|  `a|b`   |                         匹配`a`或`b`                         |\n|  `( )`   |               匹配括号内的表达式，也表示一个组               |\n\n看完了之后，可能有点晕晕的吧，不过不用担心，后面我们会详细讲解一些常见规则的用法。\n\n其实正则表达式不是Python独有的，它也可以用在其他编程语言中。但是Python的re库提供了整个正则表达式的实现，利用这个库，可以在Python中使用正则表达式。在Python中写正则表达式几乎都用这个库，下面就来了解它的一些常用方法。\n\n## 2. `match()`\n\n这里首先介绍第一个常用的匹配方法——`match()`，向它传入要匹配的字符串以及正则表达式，就可以检测这个正则表达式是否匹配字符串。\n\n`match()`方法会尝试从字符串的起始位置匹配正则表达式，如果匹配，就返回匹配成功的结果；如果不匹配，就返回`None`。示例如下：\n\n```python\nimport re \ncontent = \'Hello 123 4567 World_This is a Regex Demo\'print(len(content))\nresult = re.match(\'^Hello\\s\\d\\d\\d\\s\\d{4}\\s\\w{10}\', content)\nprint(result)\nprint(result.group())\nprint(result.span())\n```\n\n运行结果如下：\n\n```\n41<_sre.SRE_Match **object**; span=(0, 25), match=\'Hello 123 4567 World_This\'>Hello 123 4567 World_This(0, 25)\n```\n\n这里首先声明了一个字符串，其中包含英文字母、空白字符、数字等。接下来，我们写一个正则表达式：\n\n```js\n^Hello\\s\\d\\d\\d\\s\\d{4}\\s\\w{10}\n```\n\n用它来匹配这个长字符串。开头的`^`是匹配字符串的开头，也就是以`Hello`开头；然后`\\s`匹配空白字符，用来匹配目标字符串的空格；`\\d`匹配数字，3个`\\d`匹配123；然后再写1个`\\s`匹配空格；后面还有`4567`，我们其实可以依然用4个`\\d`来匹配，但是这么写比较烦琐，所以后面可以跟`{4}`以代表匹配前面的规则4次，也就是匹配4个数字；然后后面再紧接1个空白字符，最后`\\w{10}`匹配10个字母及下划线。我们注意到，这里其实并没有把目标字符串匹配完，不过这样依然可以进行匹配，只不过匹配结果短一点而已。\n\n而在`match()`方法中，第一个参数传入了正则表达式，第二个参数传入了要匹配的字符串。\n\n打印输出结果，可以看到结果是`SRE_Match`对象，这证明成功匹配。该对象有两个方法：`group()`方法可以输出匹配到的内容，结果是`Hello 123 4567 World_This`，这恰好是正则表达式规则所匹配的内容；`span()`方法可以输出匹配的范围，结果是`(0, 25)`，这就是匹配到的结果字符串在原字符串中的位置范围。\n\n通过上面的例子，我们基本了解了如何在Python中使用正则表达式来匹配一段文字。\n\n### 匹配目标\n\n刚才我们用`match()`方法可以得到匹配到的字符串内容，但是如果想从字符串中提取一部分内容，该怎么办呢？就像最前面的实例一样，从一段文本中提取出邮件或电话号码等内容。\n\n这里可以使用`()`括号将想提取的子字符串括起来。`()`实际上标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，调用`group()`方法传入分组的索引即可获取提取的结果。示例如下：\n\n```python\nimport re \ncontent = \'Hello 1234567 World_This is a Regex Demo\'\nresult = re.match(\'^Hello\\s(\\d+)\\sWorld\', content)\nprint(result)\nprint(result.group())\nprint(result.group(1))\nprint(result.span())\n```\n\n这里我们想把字符串中的`1234567`提取出来，此时可以将数字部分的正则表达式用`()`括起来，然后调用了`group(1)`获取匹配结果。\n\n运行结果如下：\n\n```\n<_sre.SRE_Match **object**; span=(0, 19), match=\'Hello 1234567 World\'>Hello 1234567 World1234567(0, 19)\n```\n\n可以看到，我们成功得到了`1234567`。这里用的是`group(1)`，它与`group()`有所不同，后者会输出完整的匹配结果，而前者会输出第一个被`()`包围的匹配结果。假如正则表达式后面还有`()`包括的内容，那么可以依次用`group(2)`、`group(3)`等来获取。\n\n### 通用匹配\n\n刚才我们写的正则表达式其实比较复杂，出现空白字符我们就写`\\s`匹配，出现数字我们就用`\\d`匹配，这样的工作量非常大。其实完全没必要这么做，因为还有一个万能匹配可以用，那就是`.*`（点星）。其中`.`（点）可以匹配任意字符（除换行符），`*`（星）代表匹配前面的字符无限次，所以它们组合在一起就可以匹配任意字符了。有了它，我们就不用挨个字符地匹配了。\n\n接着上面的例子，我们可以改写一下正则表达式：\n\n```python\nimport re \ncontent = \'Hello 123 4567 World_This is a Regex Demo\'\nresult = re.match(\'^Hello.*Demo$\', content)\nprint(result)\nprint(result.group())\nprint(result.span())\n```\n\n这里我们将中间部分直接省略，全部用`.*`来代替，最后加一个结尾字符串就好了。运行结果如下：\n\n```\n<_sre.SRE_Match **object**; span=(0, 41), match=\'Hello 123 4567 World_This is a Regex Demo\'>Hello 123 4567 World_This **is** a Regex Demo(0, 41)\n```\n\n可以看到，`group()`方法输出了匹配的全部字符串，也就是说我们写的正则表达式匹配到了目标字符串的全部内容；`span()`方法输出`(0, 41)`，这是整个字符串的长度。\n\n因此，我们可以使用`.*`简化正则表达式的书写。\n\n### 贪婪与非贪婪\n\n使用上面的通用匹配`.*`时，可能有时候匹配到的并不是我们想要的结果。看下面的例子：\n\n```python\nimport re \ncontent = \'Hello 1234567 World_This is a Regex Demo\'\nresult = re.match(\'^He.*(\\d+).*Demo$\', content)\nprint(result)\nprint(result.group(1))\n```\n\n这里我们依然想获取中间的数字，所以中间依然写的是`(\\d+)`。而数字两侧由于内容比较杂乱，所以想省略来写，都写成 `.*`。最后，组成`^He.*(\\d+).*Demo$`，看样子并没有什么问题。我们看下运行结果：\n\n```\n<_sre.SRE_Match **object**; span=(0, 40), match=\'Hello 1234567 World_This is a Regex Demo\'>7\n```\n\n奇怪的事情发生了，我们只得到了7这个数字，这是怎么回事呢？\n\n这里就涉及一个贪婪匹配与非贪婪匹配的问题了。在贪婪匹配下，`.*`会匹配尽可能多的字符。正则表达式中`.*`后面是`\\d+`，也就是至少一个数字，并没有指定具体多少个数字，因此，`.*`就尽可能匹配多的字符，这里就把`123456`匹配了，给`\\d+`留下一个可满足条件的数字7，最后得到的内容就只有数字7了。\n\n但这很明显会给我们带来很大的不便。有时候，匹配结果会莫名其妙少了一部分内容。其实，这里只需要使用非贪婪匹配就好了。非贪婪匹配的写法是`.*?`，多了一个`?`，那么它可以达到怎样的效果？我们再用实例看一下：\n\n```python\nimport re \ncontent = \'Hello 1234567 World_This is a Regex Demo\'\nresult = re.match(\'^He.*?(\\d+).*Demo$\', content)\nprint(result)\nprint(result.group(1))\n```\n\n这里我们只是将第一个`.*`改成了`.*?`，转变为非贪婪匹配。结果如下：\n\n```\n<_sre.SRE_Match **object**; span=(0, 40), match=\'Hello 1234567 World_This is a Regex Demo\'>1234567\n```\n\n此时就可以成功获取`1234567`了。原因可想而知，贪婪匹配是尽可能匹配多的字符，非贪婪匹配就是尽可能匹配少的字符。当`.*?`匹配到`Hello`后面的空白字符时，再往后的字符就是数字了，而`\\d+`恰好可以匹配，那么这里`.*?`就不再进行匹配，交给`\\d+`去匹配后面的数字。所以这样`.*?`匹配了尽可能少的字符，`\\d+`的结果就是`1234567`了。\n\n所以说，在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用`.*?`来代替`.*`，以免出现匹配结果缺失的情况。\n\n但这里需要注意，如果匹配的结果在字符串结尾，`.*?`就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。例如：\n\n```python\nimport re \ncontent = \'http://weibo.com/comment/kEraCN\'\nresult1 = re.match(\'http.*?comment/(.*?)\', content)\nresult2 = re.match(\'http.*?comment/(.*)\', content)\nprint(\'result1\', result1.group(1))\nprint(\'result2\', result2.group(1))\n```\n\n运行结果如下：\n\n```\nresult1 result2 kEraCN\n```\n\n可以观察到，`.*?`没有匹配到任何结果，而`.*`则尽量匹配多的内容，成功得到了匹配结果。\n\n### 修饰符\n\n正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。我们用实例来看一下：\n\n```python\nimport re \ncontent = \'\'\'Hello 1234567 World_Thisis a Regex Demo\'\'\'\nresult = re.match(\'^He.*?(\\d+).*?Demo$\', content)\nprint(result.group(1))\n```\n\n和上面的例子相仿，我们在字符串中加了换行符，正则表达式还是一样的，用来匹配其中的数字。看一下运行结果：\n\n```\nAttributeError Traceback (most recent call last)<ipython-input-18-c7d232b39645> **in** <module>()      5 \'\'\'      6 result = re.match(\'^He.*?(\\d+).*?Demo$\', content)----> 7 print(result.group(1)) AttributeError: \'NoneType\' object has no attribute \'group\'\n```\n\n运行直接报错，也就是说正则表达式没有匹配到这个字符串，返回结果为`None`，而我们又调用了`group()`方法导致`AttributeError`。\n\n那么，为什么加了一个换行符，就匹配不到了呢？这是因为`\\.`匹配的是除换行符之外的任意字符，当遇到换行符时，`.*?`就不能匹配了，所以导致匹配失败。这里只需加一个修饰符`re.S`，即可修正这个错误：\n\n```python\nresult = re.match(\'^He.*?(\\d+).*?Demo$\', content, re.S)\n```\n\n这个修饰符的作用是使`.`匹配包括换行符在内的所有字符。此时运行结果如下：\n\n```\n1234567\n```\n\n这个`re.S`在网页匹配中经常用到。因为HTML节点经常会有换行，加上它，就可以匹配节点与节点之间的换行了。\n\n另外，还有一些修饰符，在必要的情况下也可以使用，如表3-3所示。\n\n表3-3 修饰符\n\n| 修饰符 |                             描述                             |\n| :----: | :----------------------------------------------------------: |\n| `re.I` |                     使匹配对大小写不敏感                     |\n| `re.L` |               做本地化识别（locale-aware）匹配               |\n| `re.M` |                    多行匹配，影响`^`和`$`                    |\n| `re.S` |               使`.`匹配包括换行在内的所有字符                |\n| `re.U` | 根据Unicode字符集解析字符。这个标志影响`\\w`、`\\W`、 `\\b`和`\\B` |\n| `re.X` | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 |\n\n在网页匹配中，较为常用的有`re.S`和`re.I`。\n\n### 转义匹配\n\n我们知道正则表达式定义了许多匹配模式，如`.`匹配除换行符以外的任意字符，但是如果目标字符串里面就包含`.`，那该怎么办呢？\n\n这里就需要用到转义匹配了，示例如下：\n\n```python\nimport re \ncontent = \'(百度)www.baidu.com\'\nresult = re.match(\'\\(百度\\)www\\.baidu\\.com\', content)\nprint(result)\n```\n\n当遇到用于正则匹配模式的特殊字符时，在前面加反斜线转义一下即可。例如`.`就可以用`\\.`来匹配，运行结果如下：\n\n```\n<_sre.SRE_Match **object**; span=(0, 17), match=\'(百度)www.baidu.com\'>\n```\n\n可以看到，这里成功匹配到了原字符串。\n\n这些是写正则表达式常用的几个知识点，熟练掌握它们对后面写正则表达式匹配非常有帮助。\n\n## 3. `search()`\n\n前面提到过，`match()`方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了。我们看下面的例子：\n\n```python\nimport re \ncontent = \'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings\'\nresult = re.match(\'Hello.*?(\\d+).*?Demo\', content)\nprint(result)\n```\n\n这里的字符串以`Extra`开头，但是正则表达式以`Hello`开头，整个正则表达式是字符串的一部分，但是这样匹配是失败的。运行结果如下：\n\n```\nNone\n```\n\n因为`match()`方法在使用时需要考虑到开头的内容，这在做匹配时并不方便。它更适合用来检测某个字符串是否符合某个正则表达式的规则。\n\n这里就有另外一个方法`search()`，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。也就是说，正则表达式可以是字符串的一部分，在匹配时，`search()`方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，就返回`None`。\n\n我们把上面代码中的`match()`方法修改成`search()`，再看下运行结果：\n\n```\n<_sre.SRE_Match **object**; span=(13, 53), match=\'Hello 1234567 World_This is a Regex Demo\'>1234567\n```\n\n这时就得到了匹配结果。\n\n因此，为了匹配方便，我们可以尽量使用`search()`方法。\n\n下面再用几个实例来看看`search()`方法的用法。\n\n首先，这里有一段待匹配的HTML文本，接下来写几个正则表达式实例来实现相应信息的提取：\n\n```python\nhtml = \'\'\'<div id=\"songs-list\">    <h2 class=\"title\">经典老歌</h2>    <p class=\"introduction\">        经典老歌列表    </p>    <ul id=\"list\" class=\"list-group\">        <li data-view=\"2\">一路上有你</li>        <li data-view=\"7\">            <a href=\"/2.mp3\" singer=\"任贤齐\">沧海一声笑</a>        </li>        <li data-view=\"4\" class=\"active\">            <a href=\"/3.mp3\" singer=\"齐秦\">往事随风</a>        </li>        <li data-view=\"6\"><a href=\"/4.mp3\" singer=\"beyond\">光辉岁月</a></li>        <li data-view=\"5\"><a href=\"/5.mp3\" singer=\"陈慧琳\">记事本</a></li>        <li data-view=\"5\">            <a href=\"/6.mp3\" singer=\"邓丽君\"><i class=\"fa fa-user\"></i>但愿人长久</a>        </li>    </ul></div>\'\'\'\n```\n\n可以观察到，`ul`节点里有许多`li`节点，其中`li`节点中有的包含`a`节点，有的不包含`a`节点，`a`节点还有一些相应的属性——超链接和歌手名。\n\n首先，我们尝试提取`class`为`active`的`li`节点内部的超链接包含的歌手名和歌名，此时需要提取第三个`li`节点下`a`节点的`singer`属性和文本。\n\n此时正则表达式可以以`li`开头，然后寻找一个标志符`active`，中间的部分可以用`.*?`来匹配。接下来，要提取`singer`这个属性值，所以还需要写入`singer=\"(.*?)\"`，这里需要提取的部分用小括号括起来，以便用`group()`方法提取出来，它的两侧边界是双引号。然后还需要匹配`a`节点的文本，其中它的左边界是`>`，右边界是`</a>`。然后目标内容依然用`(.*?)`来匹配，所以最后的正则表达式就变成了：\n\n```html\n<li.*?active.*?singer=\"(.*?)\">(.*?)</a>\n```\n\n然后再调用`search()`方法，它会搜索整个HTML文本，找到符合正则表达式的第一个内容返回。\n\n另外，由于代码有换行，所以这里第三个参数需要传入`re.S`。整个匹配代码如下：\n\n```python\nresult = re.search(\'<li.*?active.*?singer=\"(.*?)\">(.*?)</a>\', html, re.S)\nif result:\n    print(result.group(1), result.group(2))\n```\n\n由于需要获取的歌手和歌名都已经用小括号包围，所以可以用`group()`方法获取。\n\n运行结果如下：\n\n```\n齐秦 往事随风\n```\n\n可以看到，这正是`class`为`active`的`li`节点内部的超链接包含的歌手名和歌名。\n\n如果正则表达式不加`active`（也就是匹配不带`class`为`active`的节点内容），那会怎样呢？我们将正则表达式中的`active`去掉，代码改写如下：\n\n```python\nresult = re.search(\'<li.*?singer=\"(.*?)\">(.*?)</a>\', html, re.S)\nif result:\n    print(result.group(1), result.group(2))\n```\n\n由于search()方法会返回第一个符合条件的匹配目标，这里结果就变了：\n\n```\n任贤齐 沧海一声笑\n```\n\n把`active`标签去掉后，从字符串开头开始搜索，此时符合条件的节点就变成了第二个`li`节点，后面的就不再匹配，所以运行结果就变成第二个`li`节点中的内容。\n\n注意，在上面的两次匹配中，`search()`方法的第三个参数都加了`re.S`，这使得`.*?`可以匹配换行，所以含有换行的`li`节点被匹配到了。如果我们将其去掉，结果会是什么？代码如下：\n\n```python\nresult = re.search(\'<li.*?singer=\"(.*?)\">(.*?)</a>\', html)\nif result:\n    print(result.group(1), result.group(2))\n```\n\n运行结果如下：\n\n```\nbeyond 光辉岁月\n```\n\n可以看到，结果变成了第四个`li`节点的内容。这是因为第二个和第三个`li`节点都包含了换行符，去掉`re.S`之后，`.*?`已经不能匹配换行符，所以正则表达式不会匹配到第二个和第三个`li`节点，而第四个`li`节点中不包含换行符，所以成功匹配。\n\n由于绝大部分的HTML文本都包含了换行符，所以尽量都需要加上`re.S`修饰符，以免出现匹配不到的问题。\n\n## 4. `findall()`\n\n前面我们介绍了`search()`方法的用法，它可以返回匹配正则表达式的第一个内容，但是如果想要获取匹配正则表达式的所有内容，那该怎么办呢？这时就要借助`findall()`方法了。该方法会搜索整个字符串，然后返回匹配正则表达式的所有内容。\n\n还是上面的HTML文本，如果想获取所有`a`节点的超链接、歌手和歌名，就可以将`search()`方法换成`findall()`方法。如果有返回结果的话，就是列表类型，所以需要遍历一下来依次获取每组内容。代码如下：\n\n```python\nresults = re.findall(\'<li.*?href=\"(.*?)\".*?singer=\"(.*?)\">(.*?)</a>\', html, re.S)\nprint(results)\nprint(type(results))\nfor result in results:\n    print(result)\n    print(result[0], result[1], result[2])\n```\n\n运行结果如下：\n\n```json\n[(\'/2.mp3\', \'任贤齐\', \'沧海一声笑\'), (\'/3.mp3\', \'齐秦\', \'往事随风\'), (\'/4.mp3\', \'beyond\', \'光辉岁月\'), (\'/5.mp3\', \'陈慧琳\', \'记事本\'), (\'/6.mp3\', \'邓丽君\', \'但愿人长久\')]<**class** \'list\'>(\'/2.mp3\', \'任贤齐\', \'沧海一声笑\')/2.mp3 任贤齐 沧海一声笑(\'/3.mp3\', \'齐秦\', \'往事随风\')/3.mp3 齐秦 往事随风(\'/4.mp3\', \'beyond\', \'光辉岁月\')/4.mp3 beyond 光辉岁月(\'/5.mp3\', \'陈慧琳\', \'记事本\')/5.mp3 陈慧琳 记事本(\'/6.mp3\', \'邓丽君\', \'但愿人长久\')/6.mp3 邓丽君 但愿人长久\n```\n\n可以看到，返回的列表中的每个元素都是元组类型，我们用对应的索引依次取出即可。\n\n如果只是获取第一个内容，可以用`search()`方法。当需要提取多个内容时，可以用`findall()`方法。\n\n## 5. `sub()`\n\n除了使用正则表达式提取信息外，有时候还需要借助它来修改文本。比如，想要把一串文本中的所有数字都去掉，如果只用字符串的`replace()`方法，那就太烦琐了，这时可以借助`sub()`方法。示例如下：\n\n```python\nimport re \ncontent = \'54aK54yr5oiR54ix5L2g\'\ncontent = re.sub(\'\\d+\', \'\', content)\nprint(content)\n```\n\n运行结果如下：\n\n```\naKyroiRixLg\n```\n\n这里只需要给第一个参数传入`\\d+`来匹配所有的数字，第二个参数为替换成的字符串（如果去掉该参数的话，可以赋值为空），第三个参数是原字符串。\n\n在上面的HTML文本中，如果想获取所有`li`节点的歌名，直接用正则表达式来提取可能比较烦琐。比如，可以写成这样子：\n\n```python\nresults = re.findall(\'<li.*?>\\s*?(<a.*?>)?(\\w+)(</a>)?\\s*?</li>\', html, re.S)\nfor result in results:\n    print(result[1])\n```\n\n运行结果如下：\n\n```\n一路上有你沧海一声笑往事随风光辉岁月记事本但愿人长久\n```\n\n此时借助`sub()`方法就比较简单了。可以先用`sub()`方法将`a`节点去掉，只留下文本，然后再利用`findall()`提取就好了：\n\n```python\nhtml = re.sub(\'<a.*?>|</a>\', \'\', html)print(html)\nresults = re.findall(\'<li.*?>(.*?)</li>\', html, re.S)\nfor result in results:\n    print(result.strip())\n```\n\n运行结果如下：\n\n```html\n<div id=\"songs-list\">    <h2 **class**=\"title\">经典老歌</h2>    <p **class**=\"introduction\">        经典老歌列表    </p>    <ul id=\"list\" **class**=\"list-group\">        <li data-view=\"2\">一路上有你</li>        <li data-view=\"7\">            沧海一声笑        </li>        <li data-view=\"4\" **class**=\"active\">            往事随风        </li>        <li data-view=\"6\">光辉岁月</li>        <li data-view=\"5\">记事本</li>        <li data-view=\"5\">            但愿人长久        </li>    </ul></div>一路上有你沧海一声笑往事随风光辉岁月记事本但愿人长久\n```\n\n可以看到，`a`节点经过`sub()`方法处理后就没有了，然后再通过`findall()`方法直接提取即可。可以看到，在适当的时候，借助`sub()`方法可以起到事半功倍的效果。\n\n## 6. `compile()`\n\n前面所讲的方法都是用来处理字符串的方法，最后再介绍一下`compile()`方法，这个方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用。示例代码如下：\n\n```python\nimport re \ncontent1 = \'2016-12-15 12:00\'\ncontent2 = \'2016-12-17 12:55\'\ncontent3 = \'2016-12-22 13:21\'\npattern = re.compile(\'\\d{2}:\\d{2}\')\nresult1 = re.sub(pattern, \'\', content1)\nresult2 = re.sub(pattern, \'\', content2)\nresult3 = re.sub(pattern, \'\', content3)\nprint(result1, result2, result3)\n```\n\n例如，这里有3个日期，我们想分别将3个日期中的时间去掉，这时可以借助`sub()`方法。该方法的第一个参数是正则表达式，但是这里没有必要重复写3个同样的正则表达式，此时可以借助`compile()`方法将正则表达式编译成一个正则表达式对象，以便复用。\n\n运行结果如下：\n\n```\n2016-12-15  2016-12-17  2016-12-22\n```\n\n另外，`compile()`还可以传入修饰符，例如`re.S`等修饰符，这样在`search()`、`findall()`等方法中就不需要额外传了。所以，`compile()`方法可以说是给正则表达式做了一层封装，以便我们更好地复用。\n\n到此为止，正则表达式的基本用法就介绍完了，后面会通过具体的实例来讲解正则表达式的用法。', 0, '本节中，我们看一下正则表达式的相关用法。正则表达式是处理字符串的强大工具，它有自己特定的语法结构，有了它，实现字符串的检索、替换、匹配验证都不在话下。\n\n当然，对于爬虫来说，有了它，从HTML里提取想要的信息就非常方便了。', '正则表达式', 0, '2019-05-06 11:07:58', 0, 1, 1);
INSERT INTO `article` VALUES (12, '本节中，我们利用requests库和正则表达式来抓取猫眼电影TOP100的相关内容。requests比urllib使用更加方便，而且目前我们还没有系统学习HTML解析库，所以这里就选用正则表达式来作为解析工具。\n\n## 1. 本节目标\n\n本节中，我们要提取出猫眼电影TOP100的电影名称、时间、评分、图片等信息，提取的站点URL为<http://maoyan.com/board/4>，提取的结果会以文件形式保存下来。\n\n## 2. 准备工作\n\n在本节开始之前，请确保已经正确安装好了requests库。如果没有安装，可以参考第1章的安装说明。\n\n## 3. 抓取分析\n\n我们需要抓取的目标站点为<http://maoyan.com/board/4>，打开之后便可以查看到榜单信息，如图3-11所示。![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-11.jpg)图3-11 榜单信息\n\n排名第一的电影是霸王别姬，页面中显示的有效信息有影片名称、主演、上映时间、上映地区、评分、图片等信息。\n\n将网页滚动到最下方，可以发现有分页的列表，直接点击第2页，观察页面的URL和内容发生了怎样的变化，如图3-12所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-12.jpg)图3-12 页面URL变化\n\n可以发现页面的URL变成<http://maoyan.com/board/4?offset=10>，比之前的URL多了一个参数，那就是`offset=10`，而目前显示的结果是排行11~20名的电影，初步推断这是一个偏移量的参数。再点击下一页，发现页面的URL变成了<http://maoyan.com/board/4?offset=20>，参数`offset`变成了20，而显示的结果是排行21~30的电影。\n\n由此可以总结出规律，`offset`代表偏移量值，如果偏移量为`n`，则显示的电影序号就是`n+1`到`n+10`，每页显示10个。所以，如果想获取TOP100电影，只需要分开请求10次，而10次的`offset`参数分别设置为0、10、20、…90即可，这样获取不同的页面之后，再用正则表达式提取出相关信息，就可以得到TOP100的所有电影信息了。\n\n## 4. 抓取首页\n\n接下来用代码实现这个过程。首先抓取第一页的内容。我们实现了`get_one_page()`方法，并给它传入`url`参数。然后将抓取的页面结果返回，再通过`main()`方法调用。初步代码实现如下：\n\n```python\nimport requests \ndef get_one_page(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.text\n    return None \n\n\ndef main():\n    url = \'http://maoyan.com/board/4\'\n    html = get_one_page(url)\n    print(html) main()\n```\n\n这样运行之后，就可以成功获取首页的源代码了。获取源代码后，就需要解析页面，提取出我们想要的信息。\n\n## 5. 正则提取\n\n接下来，回到网页看一下页面的真实源码。在开发者模式下的Network监听组件中查看源代码，如图3-13所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-13.jpg)图3-13 源代码\n\n注意，这里不要在Elements选项卡中直接查看源码，因为那里的源码可能经过JavaScript操作而与原始请求不同，而是需要从Network选项卡部分查看原始请求得到的源码。\n\n查看其中一个条目的源代码，如图3-14所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-14.jpg)图3-14 源代码\n\n可以看到，一部电影信息对应的源代码是一个`dd`节点，我们用正则表达式来提取这里面的一些电影信息。首先，需要提取它的排名信息。而它的排名信息是在`class`为`board-index`的`i`节点内，这里利用非贪婪匹配来提取`i`节点内的信息，正则表达式写为：\n\n```js\n<dd>.*?board-index.*?>(.*?)</i>\n```\n\n随后需要提取电影的图片。可以看到，后面有`a`节点，其内部有两个`img`节点。经过检查后发现，第二个`img`节点的`data-src`属性是图片的链接。这里提取第二个`img`节点的`data-src`属性，正则表达式可以改写如下：\n\n```js\n<dd>.*?board-index.*?>(.*?)</i>.*?data-src=\"(.*?)\"\n```\n\n再往后，需要提取电影的名称，它在后面的`p`节点内，`class`为`name`。所以，可以用`name`做一个标志位，然后进一步提取到其内`a`节点的正文内容，此时正则表达式改写如下：\n\n```js\n<dd>.*?board-index.*?>(.*?)</i>.*?data-src=\"(.*?)\".*?name.*?a.*?>(.*?)</a>\n```\n\n再提取主演、发布时间、评分等内容时，都是同样的原理。最后，正则表达式写为：\n\n```js\n<dd>.*?board-index.*?>(.*?)</i>.*?data-src=\"(.*?)\".*?name.*?a.*?>(.*?)</a>.*?star.*?>(.*?)</p>.*?releasetime.*?>(.*?)</p>.*?**integer**.*?>(.*?)</i>.*?fraction.*?>(.*?)</i>.*?</dd>\n```\n\n这样一个正则表达式可以匹配一个电影的结果，里面匹配了7个信息。接下来，通过调用`findall()`方法提取出所有的内容。\n\n接下来，我们再定义解析页面的方法`parse_one_page()`，主要是通过正则表达式来从结果中提取出我们想要的内容，实现代码如下：\n\n```python\ndef parse_one_page(html):\n    pattern = re.compile(        \'<dd>.*?board-index.*?>(.*?)</i>.*?data-src=\"(.*?)\".*?name.*?a.*?>(.*?)</a>.*?star.*?>(.*?)</p>.*?releasetime.*?>(.*?)</p>.*?integer.*?>(.*?)</i>.*?fraction.*?>(.*?)</i>.*?</dd>\',        re.S) \n    items = re.findall(pattern, html) \n    print(items)\n```\n\n这样就可以成功地将一页的10个电影信息都提取出来，这是一个列表形式，输出结果如下：\n\n```json\n[(\'1\', \'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c\', \'霸王别姬\',\n    \'\\n                主演：张国荣,张丰毅,巩俐\\n        \', \'上映时间：1993-01-01(中国香港)\', \'9.\', \'6\'), (\'2\',\n    \'http://p0.meituan.net/movie/__40191813__4767047.jpg@160w_220h_1e_1c\', \'肖申克的救赎\',\n    \'\\n                主演：蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿\\n        \', \'上映时间：1994-10-14(美国)\', \'9.\', \'5\'), (\'3\',\n    \'http://p0.meituan.net/movie/fc9d78dd2ce84d20e53b6d1ae2eea4fb1515304.jpg@160w_220h_1e_1c\', \'这个杀手不太冷\',\n    \'\\n                主演：让·雷诺,加里·奥德曼,娜塔莉·波特曼\\n        \', \'上映时间：1994-09-14(法国)\', \'9.\', \'5\'), (\'4\',\n    \'http://p0.meituan.net/movie/23/6009725.jpg@160w_220h_1e_1c\', \'罗马假日\',\n    \'\\n                主演：格利高利·派克,奥黛丽·赫本,埃迪·艾伯特\\n        \', \'上映时间：1953-09-02(美国)\', \'9.\', \'1\'), (\'5\',\n    \'http://p0.meituan.net/movie/53/1541925.jpg@160w_220h_1e_1c\', \'阿甘正传\',\n    \'\\n                主演：汤姆·汉克斯,罗宾·怀特,加里·西尼斯\\n        \', \'上映时间：1994-07-06(美国)\', \'9.\', \'4\'), (\'6\',\n    \'http://p0.meituan.net/movie/11/324629.jpg@160w_220h_1e_1c\', \'泰坦尼克号\',\n    \'\\n                主演：莱昂纳多·迪卡普里奥,凯特·温丝莱特,比利·赞恩\\n        \', \'上映时间：1998-04-03\', \'9.\', \'5\'), (\'7\',\n    \'http://p0.meituan.net/movie/99/678407.jpg@160w_220h_1e_1c\', \'龙猫\',\n    \'\\n                主演：日高法子,坂本千夏,糸井重里\\n        \', \'上映时间：1988-04-16(日本)\', \'9.\', \'2\'), (\'8\',\n    \'http://p0.meituan.net/movie/92/8212889.jpg@160w_220h_1e_1c\', \'教父\',\n    \'\\n                主演：马龙·白兰度,阿尔·帕西诺,詹姆斯·凯恩\\n        \', \'上映时间：1972-03-24(美国)\', \'9.\', \'3\'), (\'9\',\n    \'http://p0.meituan.net/movie/62/109878.jpg@160w_220h_1e_1c\', \'唐伯虎点秋香\',\n    \'\\n                主演：周星驰,巩俐,郑佩佩\\n        \', \'上映时间：1993-07-01(中国香港)\', \'9.\', \'2\'), (\'10\',\n    \'http://p0.meituan.net/movie/9bf7d7b81001a9cf8adbac5a7cf7d766132425.jpg@160w_220h_1e_1c\', \'千与千寻\',\n    \'\\n                主演：柊瑠美,入野自由,夏木真理\\n        \', \'上映时间：2001-07-20(日本)\', \'9.\', \'3\')]\n```\n\n但这样还不够，数据比较杂乱，我们再将匹配结果处理一下，遍历提取结果并生成字典，此时方法改写如下：\n\n```python\ndef parse_one_page(html):\n    pattern = re.compile(        \'<dd>.*?board-index.*?>(.*?)</i>.*?data-src=\"(.*?)\".*?name.*?a.*?>(.*?)</a>.*?star.*?>(.*?)</p>.*?releasetime.*?>(.*?)</p>.*?integer.*?>(.*?)</i>.*?fraction.*?>(.*?)</i>.*?</dd>\',        re.S)\n    items = re.findall(pattern, html)\n    for item in items:\n        yield {\n            \'index\': item[0],\n            \'image\': item[1], \n            \'title\': item[2].strip(),\n            \'actor\': item[3].strip()[3:]\n            if len(item[3]) > 3 else \'\',\n            \'time\': item[4].strip()[5:] \n            if len(item[4]) > 5 else \'\',\n            \'score\': item[5].strip() + item[6].strip() \n        }\n```\n\n这样就可以成功提取出电影的排名、图片、标题、演员、时间、评分等内容了，并把它赋值为一个个的字典，形成结构化数据。运行结果如下：\n\n```json\n{\n    \'image\': \'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c\',\n    \'actor\': \'张国荣,张丰毅,巩俐\',\n    \'score\': \'9.6\',\n    \'index\': \'1\',\n    \'title\': \'霸王别姬\',\n    \'time\': \'1993-01-01(中国香港)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/__40191813__4767047.jpg@160w_220h_1e_1c\',\n    \'actor\': \'蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿\',\n    \'score\': \'9.5\',\n    \'index\': \'2\',\n    \'title\': \'肖申克的救赎\',\n    \'time\': \'1994-10-14(美国)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/fc9d78dd2ce84d20e53b6d1ae2eea4fb1515304.jpg@160w_220h_1e_1c\',\n    \'actor\': \'让·雷诺,加里·奥德曼,娜塔莉·波特曼\',\n    \'score\': \'9.5\',\n    \'index\': \'3\',\n    \'title\': \'这个杀手不太冷\',\n    \'time\': \'1994-09-14(法国)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/23/6009725.jpg@160w_220h_1e_1c\',\n    \'actor\': \'格利高利·派克,奥黛丽·赫本,埃迪·艾伯特\',\n    \'score\': \'9.1\',\n    \'index\': \'4\',\n    \'title\': \'罗马假日\',\n    \'time\': \'1953-09-02(美国)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/53/1541925.jpg@160w_220h_1e_1c\',\n    \'actor\': \'汤姆·汉克斯,罗宾·怀特,加里·西尼斯\',\n    \'score\': \'9.4\',\n    \'index\': \'5\',\n    \'title\': \'阿甘正传\',\n    \'time\': \'1994-07-06(美国)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/11/324629.jpg@160w_220h_1e_1c\',\n    \'actor\': \'莱昂纳多·迪卡普里奥,凯特·温丝莱特,比利·赞恩\',\n    \'score\': \'9.5\',\n    \'index\': \'6\',\n    \'title\': \'泰坦尼克号\',\n    \'time\': \'1998-04-03\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/99/678407.jpg@160w_220h_1e_1c\',\n    \'actor\': \'日高法子,坂本千夏,糸井重里\',\n    \'score\': \'9.2\',\n    \'index\': \'7\',\n    \'title\': \'龙猫\',\n    \'time\': \'1988-04-16(日本)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/92/8212889.jpg@160w_220h_1e_1c\',\n    \'actor\': \'马龙·白兰度,阿尔·帕西诺,詹姆斯·凯恩\',\n    \'score\': \'9.3\',\n    \'index\': \'8\',\n    \'title\': \'教父\',\n    \'time\': \'1972-03-24(美国)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/62/109878.jpg@160w_220h_1e_1c\',\n    \'actor\': \'周星驰,巩俐,郑佩佩\',\n    \'score\': \'9.2\',\n    \'index\': \'9\',\n    \'title\': \'唐伯虎点秋香\',\n    \'time\': \'1993-07-01(中国香港)\'\n} {\n    \'image\': \'http://p0.meituan.net/movie/9bf7d7b81001a9cf8adbac5a7cf7d766132425.jpg@160w_220h_1e_1c\',\n    \'actor\': \'柊瑠美,入野自由,夏木真理\',\n    \'score\': \'9.3\',\n    \'index\': \'10\',\n    \'title\': \'千与千寻\',\n    \'time\': \'2001-07-20(日本)\'\n}\n```\n\n到此为止，我们就成功提取了单页的电影信息。\n\n## 6. 写入文件\n\n随后，我们将提取的结果写入文件，这里直接写入到一个文本文件中。这里通过JSON库的`dumps()`方法实现字典的序列化，并指定`ensure_ascii`参数为`False`，这样可以保证输出结果是中文形式而不是Unicode编码。代码如下：\n\n```python\ndef write_to_json(content):\n    with open(\'result.txt\', \'a\') as f:\n        print(type(json.dumps(content)))\n        f.write(json.dumps(content, ensure_ascii=**False**,).encode(\'utf-8\'))\n```\n\n通过调用`write_to_json()`方法即可实现将字典写入到文本文件的过程，此处的`content`参数就是一部电影的提取结果，是一个字典。\n\n## 7. 整合代码\n\n最后，实现`main()`方法来调用前面实现的方法，将单页的电影结果写入到文件。相关代码如下：\n\n```python\ndef main():\n    url = \'http://maoyan.com/board/4\'\n    html = get_one_page(url)\n    for item in parse_one_page(html):\n        write_to_json(item)\n```\n\n到此为止，我们就完成了单页电影的提取，也就是首页的10部电影可以成功提取并保存到文本文件中了。\n\n## 8. 分页爬取\n\n因为我们需要抓取的是TOP100的电影，所以还需要遍历一下，给这个链接传入`offset`参数，实现其他90部电影的爬取，此时添加如下调用即可：\n\n```python\nif __name__ == \'__main__\':\n    for i in range(10):\n        main(offset=i * 10)\n```\n\n这里还需要将`main()`方法修改一下，接收一个`offset`值作为偏移量，然后构造URL进行爬取。实现代码如下：\n\n```python\ndef main(offset):\n    url = \'http://maoyan.com/board/4?offset=\' + str(offset)\n    html = get_one_page(url)\n    for item in parse_one_page(html):\n        print(item)\n        write_to_file(item)\n```\n\n到此为止，我们的猫眼电影TOP100的爬虫就全部完成了，再稍微整理一下，完整的代码如下：\n\n```python\nimport jsonimport requestsfrom requests.exceptions\nimport RequestExceptionimport reimport time def get_one_page(url):\n    try: response = requests.get(url)\nif response.status_code == 200:\n    return response.text\nreturn None except RequestException:\n    return None def parse_one_page(html): pattern = re.compile(\'<dd>.*?board-index.*?>(\\d+)</i>.*?data-src=\"(.*?)\".*?name\"><a\' + \'.*?>(.*?)</a>.*?star\">(.*?)</p>.*?releasetime\">(.*?)</p>\' + \'.*?integer\">(.*?)</i>.*?fraction\">(.*?)</i>.*?</dd>\', re.S) items = re.findall(pattern, html) * *\n        for item in items: yield {\n            \'index\': item[0],\n            \'image\': item[1],\n            \'title\': item[2],\n            \'actor\': item[3].strip()[3: ],\n            \'time\': item[4].strip()[5: ],\n            \'score\': item[5] + item[6]\n        }\ndef write_to_file(content): with open(\'result.txt\', \'a\', encoding = \'utf-8\') as f: f.write(json.dumps(content, ensure_ascii = False) + \'\\n\') def main(offset): url = \'http://maoyan.com/board/4?offset=\' + str(offset) html = get_one_page(url) * *\n    for item in parse_one_page(html): print(item) write_to_file(item)\nif __name__ == \'__main__\':\n    for i in range(10): main(offset = i * 10) time.sleep(1)\n```\n\n现在猫眼多了反爬虫，如果速度过快，则会无响应，所以这里又增加了一个延时等待。\n\n## 9. 运行结果\n\n最后，我们运行一下代码，输出结果类似如下：\n\n```json\n{\n    \'index\': \'1\',\n    \'image\': \'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c\',\n    \'title\': \'霸王别姬\',\n    \'actor\': \'张国荣,张丰毅,巩俐\',\n    \'time\': \'1993-01-01(中国香港)\',\n    \'score\': \'9.6\'\n} {\n    \'index\': \'2\',\n    \'image\': \'http://p0.meituan.net/movie/__40191813__4767047.jpg@160w_220h_1e_1c\',\n    \'title\': \'肖申克的救赎\',\n    \'actor\': \'蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿\',\n    \'time\': \'1994-10-14(美国)\',\n    \'score\': \'9.5\'\n}...{\n    \'index\': \'98\',\n    \'image\': \'http://p0.meituan.net/movie/76/7073389.jpg@160w_220h_1e_1c\',\n    \'title\': \'东京物语\',\n    \'actor\': \'笠智众,原节子,杉村春子\',\n    \'time\': \'1953-11-03(日本)\',\n    \'score\': \'9.1\'\n} {\n    \'index\': \'99\',\n    \'image\': \'http://p0.meituan.net/movie/52/3420293.jpg@160w_220h_1e_1c\',\n    \'title\': \'我爱你\',\n    \'actor\': \'宋在河,李彩恩,吉海延\',\n    \'time\': \'2011-02-17(韩国)\',\n    \'score\': \'9.0\'\n} {\n    \'index\': \'100\',\n    \'image\': \'http://p1.meituan.net/movie/__44335138__8470779.jpg@160w_220h_1e_1c\',\n    \'title\': \'迁徙的鸟\',\n    \'actor\': \'雅克·贝汉,菲利普·拉波洛,Philippe Labro\',\n    \'time\': \'2001-12-12(法国)\',\n    \'score\': \'9.1\'\n}\n```\n\n这里省略了中间的部分输出结果。可以看到，这样就成功地把TOP100的电影信息爬取下来了。\n\n这时我们再看下文本文件，结果如图3-15所示。\n\n![img](https://qiniu.cuiqingcai.com/wp-content/uploads/2018/02/3-15.jpg)图3-15 运行结果\n\n可以看到，电影信息也已全部保存到了文本文件中了，大功告成！', 0, '本节中，我们利用requests库和正则表达式来抓取猫眼电影TOP100的相关内容。requests比urllib使用更加方便，而且目前我们还没有系统学习HTML解析库，所以这里就选用正则表达式来作为解析工具。', '抓取猫眼电影数据', 0, '2019-05-06 13:59:57', 1, 1, 1);
INSERT INTO `article` VALUES (13, '# 前言\n我开发这个博客系统的初衷是提供一个校内的平台让师生可以在这里探讨学习上的问题，总结自己在学习中遇到的难点或者将自己的理解分享给大家。\n# 怎么用\n本系统的功能从大的方面来说只有两个，一是**学生发表文章**，文章的内容不限，你可以用它总结知识点，也可以记录生活中的一些感想，并把它分享给校内的小伙伴。二是**老师创建话题**，学生可以在该话题下面分享自己的理解。举个例子来说，老师这节课讲到《海的女儿》这篇课文，并创建了同名的一个话题，同学们就可以在该话题下发表文章，例如读后感、自己重新构思的结局等等。总之具体功能已经实现，怎样去玩全靠同学们的想象力了。\n\n下面详细介绍如何创作一篇文章或者话题。\n## 1、Markdown介绍\n文章内容和话题简介都是采用Markdown语法解析的，相信很多人还并不熟悉这个名词，但它的应用已经越来越广泛了，比如Github上每个Repository的介绍就是由Markdown语法来生成的，它的目标就是使人们能够专注于写作，而不用特别考虑格式的问题。\n\n而且Markdown语法的学习门槛并不高，只需要简单地了解常用的语法就可以写出图文并茂的文章。推荐大家在[http://www.markdown.cn/](http://www.markdown.cn/)了解一下使用方式。\n## 2、创作\n首先点击侧边栏的创建\n![](https://i.loli.net/2019/05/06/5ccfc75cb307e.jpg)\n然后在左侧编写自己的文章，写完之后在右上角点击保存按钮。\n![](https://i.loli.net/2019/05/06/5ccfc9d3dc5ed.jpg)\n然后会弹出保存对话框，如果勾选归类到话题则会在相应的话题中显示当前文章，如果想要给文章增加标签的话需要进入**侧边栏-我的-标签**界面增加标签之后才会在标签列表中出现。\n![](https://i.loli.net/2019/05/06/5ccfcaac50629.jpg)\n# 总结\n由于整个系统完全由我一人编写，界面可能不是特别美观，功能上也有一些需要改进的地方。如果给大家带来不便之处，希望大家能够谅解。\n\n最后，祝大家学习进步，生活愉快！', 0, '介绍了本系统的基本用法。', '使用说明', 1, '2019-05-06 13:56:59', 11, 1, NULL);

-- ----------------------------
-- Table structure for article_tag
-- ----------------------------
DROP TABLE IF EXISTS `article_tag`;
CREATE TABLE `article_tag`  (
  `article_id` bigint(20) NOT NULL,
  `tag_id` bigint(20) NOT NULL,
  INDEX `FKesqp7s9jj2wumlnhssbme5ule`(`tag_id`) USING BTREE,
  INDEX `FKenqeees0y8hkm7x1p1ittuuye`(`article_id`) USING BTREE,
  CONSTRAINT `FKenqeees0y8hkm7x1p1ittuuye` FOREIGN KEY (`article_id`) REFERENCES `article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKesqp7s9jj2wumlnhssbme5ule` FOREIGN KEY (`tag_id`) REFERENCES `tag` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of article_tag
-- ----------------------------
INSERT INTO `article_tag` VALUES (1, 1);
INSERT INTO `article_tag` VALUES (2, 1);
INSERT INTO `article_tag` VALUES (3, 1);
INSERT INTO `article_tag` VALUES (4, 1);
INSERT INTO `article_tag` VALUES (5, 1);
INSERT INTO `article_tag` VALUES (6, 1);
INSERT INTO `article_tag` VALUES (7, 1);
INSERT INTO `article_tag` VALUES (8, 1);
INSERT INTO `article_tag` VALUES (9, 1);
INSERT INTO `article_tag` VALUES (10, 1);
INSERT INTO `article_tag` VALUES (11, 1);
INSERT INTO `article_tag` VALUES (12, 1);

-- ----------------------------
-- Table structure for article_user
-- ----------------------------
DROP TABLE IF EXISTS `article_user`;
CREATE TABLE `article_user`  (
  `article_id` bigint(20) NOT NULL,
  `user_id` bigint(20) NOT NULL,
  INDEX `FK8xe46t7m6xoph406ppemds7s8`(`user_id`) USING BTREE,
  INDEX `FKso1jvk1hrd536jf99fowobs3n`(`article_id`) USING BTREE,
  CONSTRAINT `FKso1jvk1hrd536jf99fowobs3n` FOREIGN KEY (`article_id`) REFERENCES `article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FK8xe46t7m6xoph406ppemds7s8` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of article_user
-- ----------------------------
INSERT INTO `article_user` VALUES (13, 1);

-- ----------------------------
-- Table structure for comment
-- ----------------------------
DROP TABLE IF EXISTS `comment`;
CREATE TABLE `comment`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `content` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `type` int(11) NOT NULL,
  `upt` datetime NOT NULL,
  `article_id` bigint(20) NULL DEFAULT NULL,
  `creator` bigint(20) NOT NULL,
  `p_comment` bigint(20) NULL DEFAULT NULL,
  `topic_id` bigint(20) NULL DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `FK5yx0uphgjc6ik6hb82kkw501y`(`article_id`) USING BTREE,
  INDEX `FKfqymi934nige0v1gba0k0cx3g`(`creator`) USING BTREE,
  INDEX `FKfnd827747oh3lhla14rbu6qsv`(`p_comment`) USING BTREE,
  INDEX `FKo3bvevu9ua4w6f8qu2b177f16`(`topic_id`) USING BTREE,
  CONSTRAINT `FKo3bvevu9ua4w6f8qu2b177f16` FOREIGN KEY (`topic_id`) REFERENCES `topic` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FK5yx0uphgjc6ik6hb82kkw501y` FOREIGN KEY (`article_id`) REFERENCES `article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKfnd827747oh3lhla14rbu6qsv` FOREIGN KEY (`p_comment`) REFERENCES `comment` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKfqymi934nige0v1gba0k0cx3g` FOREIGN KEY (`creator`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of comment
-- ----------------------------
INSERT INTO `comment` VALUES (1, '感谢大家帮助我测试~', 0, '2019-05-06 17:03:16', 13, 1, NULL, NULL);
INSERT INTO `comment` VALUES (2, '通过爬取猫眼电影的例子，大家可以举一反三尝试爬取豆瓣电影的数据。', 1, '2019-05-06 17:06:04', NULL, 1, NULL, 1);

-- ----------------------------
-- Table structure for message
-- ----------------------------
DROP TABLE IF EXISTS `message`;
CREATE TABLE `message`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `content` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `isread` int(11) NOT NULL,
  `type` int(11) NOT NULL,
  `upt` datetime NOT NULL,
  `creator` bigint(20) NULL DEFAULT NULL,
  `target` bigint(20) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `FKsc9iqfqtb1lgidvx67f8gl84d`(`creator`) USING BTREE,
  INDEX `FK15edrihe6axcp55lasraa2o1r`(`target`) USING BTREE,
  CONSTRAINT `FK15edrihe6axcp55lasraa2o1r` FOREIGN KEY (`target`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKsc9iqfqtb1lgidvx67f8gl84d` FOREIGN KEY (`creator`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of message
-- ----------------------------
INSERT INTO `message` VALUES (1, ' 评论了你的文章:《<a style=\"color:rgb(64, 158, 255)\" href=\"/#/browser/essay/detail/13\">使用说明</a>》', 1, 0, '2019-05-06 17:03:16', 1, 1);
INSERT INTO `message` VALUES (2, ' 评论了你的话题:《<a style=\"color:rgb(64, 158, 255)\" href=\"/#/browser/topic/detail/1\">Python制作简单爬虫</a>》', 1, 0, '2019-05-06 17:06:04', 1, 1);

-- ----------------------------
-- Table structure for resource
-- ----------------------------
DROP TABLE IF EXISTS `resource`;
CREATE TABLE `resource`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `path` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `size` bigint(20) NOT NULL,
  `upt` datetime NOT NULL,
  `url` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `uploader` bigint(20) NULL DEFAULT NULL,
  `article` bigint(20) NULL DEFAULT NULL,
  `topic` bigint(20) NULL DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `FKe7l0fo3hdetwte7gnkkwtfks`(`uploader`) USING BTREE,
  INDEX `FK7eglcfforhx5n6538b4e8dlww`(`article`) USING BTREE,
  INDEX `FK9bsssa5djbaikavnlrjd0cxit`(`topic`) USING BTREE,
  CONSTRAINT `FK9bsssa5djbaikavnlrjd0cxit` FOREIGN KEY (`topic`) REFERENCES `topic` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FK7eglcfforhx5n6538b4e8dlww` FOREIGN KEY (`article`) REFERENCES `article` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKe7l0fo3hdetwte7gnkkwtfks` FOREIGN KEY (`uploader`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB AUTO_INCREMENT = 3 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Table structure for tag
-- ----------------------------
DROP TABLE IF EXISTS `tag`;
CREATE TABLE `tag`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `background` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `color` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `creator` bigint(20) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `FKjub7d6y5o9xvaihc9u2s2d181`(`creator`) USING BTREE,
  CONSTRAINT `FKjub7d6y5o9xvaihc9u2s2d181` FOREIGN KEY (`creator`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of tag
-- ----------------------------
INSERT INTO `tag` VALUES (1, '#ECF5FF', '#669EFF', 'Python', 1);

-- ----------------------------
-- Table structure for topic
-- ----------------------------
DROP TABLE IF EXISTS `topic`;
CREATE TABLE `topic`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `summary` longtext CHARACTER SET utf8 COLLATE utf8_general_ci NULL,
  `upt` datetime NOT NULL,
  `creator` bigint(20) NOT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `FK7u0oinx2926gyl6tnsnhtptw9`(`creator`) USING BTREE,
  CONSTRAINT `FK7u0oinx2926gyl6tnsnhtptw9` FOREIGN KEY (`creator`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of topic
-- ----------------------------
INSERT INTO `topic` VALUES (1, 'Python制作简单爬虫', '人生苦短，我用Python。\n\n这句话在程序员圈子里面应该是比较常听到的，也自然有它的道理。Python最广为人知的一个功能就是制作爬虫，最基础的爬虫就是爬取网站上的数据，例如电影网站的电影票房数据，12306的车次信息等等。\n\n学会制作爬虫，将会使你的工作学习变得更加轻松。本话题下面的文章，大部分截取自[Python3网络爬虫开发实战教程](https://cuiqingcai.com/5052.html)，如对Python有更深兴趣的，可以在该网站进行全面的学习。', '2019-05-06 10:57:54', 1);

-- ----------------------------
-- Table structure for topic_tag
-- ----------------------------
DROP TABLE IF EXISTS `topic_tag`;
CREATE TABLE `topic_tag`  (
  `topic_id` bigint(20) NOT NULL,
  `tag_id` bigint(20) NOT NULL,
  INDEX `FKghqd3bga4hj3pklcwmgw36f9l`(`tag_id`) USING BTREE,
  INDEX `FKqvqmd2eomy749a9uei56n71rd`(`topic_id`) USING BTREE,
  CONSTRAINT `FKqvqmd2eomy749a9uei56n71rd` FOREIGN KEY (`topic_id`) REFERENCES `topic` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKghqd3bga4hj3pklcwmgw36f9l` FOREIGN KEY (`tag_id`) REFERENCES `tag` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of topic_tag
-- ----------------------------
INSERT INTO `topic_tag` VALUES (1, 1);

-- ----------------------------
-- Table structure for topic_user
-- ----------------------------
DROP TABLE IF EXISTS `topic_user`;
CREATE TABLE `topic_user`  (
  `topic_id` bigint(20) NOT NULL,
  `user_id` bigint(20) NOT NULL,
  INDEX `FKga75s0q7bp80rcqx7lqvjcm8k`(`user_id`) USING BTREE,
  INDEX `FKdd1vsk5b3sb9i9j7kwhpomhyc`(`topic_id`) USING BTREE,
  CONSTRAINT `FKdd1vsk5b3sb9i9j7kwhpomhyc` FOREIGN KEY (`topic_id`) REFERENCES `topic` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKga75s0q7bp80rcqx7lqvjcm8k` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Table structure for user
-- ----------------------------
DROP TABLE IF EXISTS `user`;
CREATE TABLE `user`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `college` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `headimg` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `hobby` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `loginname` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `mail` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `password` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `pro` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `real_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `reg` datetime NOT NULL,
  `role` int(11) NOT NULL,
  `sex` int(11) NULL DEFAULT NULL,
  `sign` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `tel` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  UNIQUE INDEX `UK_i3rcdsc1j905jadocmdj4ikyq`(`loginname`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 90 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

-- ----------------------------
-- Records of user
-- ----------------------------
INSERT INTO `user` VALUES (1, '信息工程系', 'https://robohash.org/necessitatibusdebitisinventore.png?size=80x80&set=set1', '编程、听歌、看电影', '1001', '980742324@qq.com', '96E79218965EB72C92A549DD5A330112', '15网工', '郑博', '2019-05-02 15:34:23', 2, 0, NULL, '13034207579');
INSERT INTO `user` VALUES (48, '信息工程系', 'https://robohash.org/necessitatibusdebitisinventore.png?size=80x80&set=set1', NULL, '160301010101', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '姜雨祥', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (49, '信息工程系', 'https://robohash.org/voluptatesconsequaturea.bmp?size=80x80&set=set1', NULL, '160301010102', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '赵彦虎', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (50, '信息工程系', 'https://robohash.org/estoccaecatieveniet.bmp?size=80x80&set=set1', NULL, '160301010103', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '方涛涛', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (51, '信息工程系', 'https://robohash.org/molestiaeahic.bmp?size=80x80&set=set1', NULL, '160301010104', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '何丽芳', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (52, '信息工程系', 'https://robohash.org/fugarepellatincidunt.bmp?size=80x80&set=set1', NULL, '160301010105', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '寇鹏飞', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (53, '信息工程系', 'https://robohash.org/sintautaliquid.jpg?size=80x80&set=set1', NULL, '160301010106', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '孙学成', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (54, '信息工程系', 'https://robohash.org/explicabocumquaerat.bmp?size=80x80&set=set1', NULL, '160301010107', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '万齐恩', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (55, '信息工程系', 'https://robohash.org/voluptatibussintsed.bmp?size=80x80&set=set1', NULL, '160301010108', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '周飞飞', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (56, '信息工程系', 'https://robohash.org/earumadipiscineque.jpg?size=80x80&set=set1', NULL, '160301010109', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '张元奎', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (57, '信息工程系', 'https://robohash.org/autenimminus.png?size=80x80&set=set1', NULL, '160301010110', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '罗正涛', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (58, '信息工程系', 'https://robohash.org/utfaceremaiores.png?size=80x80&set=set1', NULL, '160301010111', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '张阳', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (59, '信息工程系', 'https://robohash.org/omnissintsuscipit.jpg?size=80x80&set=set1', NULL, '160301010112', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '郑奇枫', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (60, '信息工程系', 'https://robohash.org/evenietmagniqui.bmp?size=80x80&set=set1', NULL, '160301010113', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '付鹏', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (61, '信息工程系', 'https://robohash.org/eadolorumqui.bmp?size=80x80&set=set1', NULL, '160301010115', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '刘伟', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (62, '信息工程系', 'https://robohash.org/rationeestsaepe.jpg?size=80x80&set=set1', NULL, '160301010116', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '曹嘉朋', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (63, '信息工程系', 'https://robohash.org/etlaudantiumipsum.bmp?size=80x80&set=set1', NULL, '160301010117', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '饶豪文', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (64, '信息工程系', 'https://robohash.org/autetest.bmp?size=80x80&set=set1', NULL, '160301010118', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '魏潇平', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (65, '信息工程系', 'https://robohash.org/enimeaet.jpg?size=80x80&set=set1', NULL, '160301010119', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '夏佩珠', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (66, '信息工程系', 'https://robohash.org/excepturirepellendusea.jpg?size=80x80&set=set1', NULL, '160301010120', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '陈小斌', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (67, '信息工程系', 'https://robohash.org/nonvoluptatemducimus.jpg?size=80x80&set=set1', NULL, '160301010121', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '黄长聪', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (68, '信息工程系', 'https://robohash.org/quinequeminus.bmp?size=80x80&set=set1', NULL, '160301010122', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '张鹏', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (69, '信息工程系', 'https://robohash.org/suscipitnumquamharum.png?size=80x80&set=set1', NULL, '160301010123', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '上官伟', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (70, '信息工程系', 'https://robohash.org/accusantiumeaut.jpg?size=80x80&set=set1', NULL, '160301010124', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '郭丹青', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (71, '信息工程系', 'https://robohash.org/quiillodolores.bmp?size=80x80&set=set1', NULL, '160301010125', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '熊慧敏', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (72, '信息工程系', 'https://robohash.org/consequaturaccusamusquae.png?size=80x80&set=set1', NULL, '160301010126', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '昌建', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (73, '信息工程系', 'https://robohash.org/doloromnisfugit.bmp?size=80x80&set=set1', NULL, '160301010127', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '钟兰兰', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (74, '信息工程系', 'https://robohash.org/exercitationemeumrerum.png?size=80x80&set=set1', NULL, '160301010128', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '张容涵', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (75, '信息工程系', 'https://robohash.org/assumendalaboremollitia.jpg?size=80x80&set=set1', NULL, '160301010129', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '肖俊杰', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (76, '信息工程系', 'https://robohash.org/earumutest.bmp?size=80x80&set=set1', NULL, '160301010130', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '李江峰', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (77, '信息工程系', 'https://robohash.org/aliquammagniexcepturi.jpg?size=80x80&set=set1', NULL, '160301010131', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '韩莉莉', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (78, '信息工程系', 'https://robohash.org/minusnecessitatibusut.bmp?size=80x80&set=set1', NULL, '160301010132', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '朱玉霜', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (79, '信息工程系', 'https://robohash.org/errordelenitivelit.png?size=80x80&set=set1', NULL, '160301010133', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '赵冰峰', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (80, '信息工程系', 'https://robohash.org/quiaestfacere.png?size=80x80&set=set1', NULL, '160301010134', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '沈翕遵', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (81, '信息工程系', 'https://robohash.org/corporisinciduntdolore.bmp?size=80x80&set=set1', NULL, '160301010135', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '梁盛禹', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (82, '信息工程系', 'https://robohash.org/possimusaliquamaut.png?size=80x80&set=set1', NULL, '160301010136', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '陈国骏', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (83, '信息工程系', 'https://robohash.org/possimusblanditiisrepudiandae.bmp?size=80x80&set=set1', NULL, '160301010137', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '查隆熙', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (84, '信息工程系', 'https://robohash.org/doloresetexpedita.png?size=80x80&set=set1', NULL, '160301010138', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '卢丽碗', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (85, '信息工程系', 'https://robohash.org/repellatsedvoluptas.png?size=80x80&set=set1', NULL, '160301010139', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '陈舒萍', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (86, '信息工程系', 'https://robohash.org/voluptatemcumquesit.bmp?size=80x80&set=set1', NULL, '160301010140', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '章检伟', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (87, '信息工程系', 'https://robohash.org/nequearchitectoodit.png?size=80x80&set=set1', NULL, '160301010141', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '钟阳', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (88, '信息工程系', 'https://robohash.org/doloroccaecatibeatae.jpg?size=80x80&set=set1', NULL, '160301010142', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '牛巍', '2019-05-06 17:21:13', 0, 0, NULL, NULL);
INSERT INTO `user` VALUES (89, '信息工程系', 'https://robohash.org/minusharumplaceat.jpg?size=80x80&set=set1', NULL, '160301010143', NULL, '96E79218965EB72C92A549DD5A330112', '16计科', '李润杰', '2019-05-06 17:21:13', 0, 0, NULL, NULL);

-- ----------------------------
-- Table structure for user_follow
-- ----------------------------
DROP TABLE IF EXISTS `user_follow`;
CREATE TABLE `user_follow`  (
  `tofws_id` bigint(20) NOT NULL,
  `befws_id` bigint(20) NOT NULL,
  INDEX `FKj1kgm3hc53643w0hu3su4aags`(`befws_id`) USING BTREE,
  INDEX `FKohko3rdyjdcrqqcwww4nwwlqv`(`tofws_id`) USING BTREE,
  CONSTRAINT `FKohko3rdyjdcrqqcwww4nwwlqv` FOREIGN KEY (`tofws_id`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT,
  CONSTRAINT `FKj1kgm3hc53643w0hu3su4aags` FOREIGN KEY (`befws_id`) REFERENCES `user` (`id`) ON DELETE RESTRICT ON UPDATE RESTRICT
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;

SET FOREIGN_KEY_CHECKS = 1;
